{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHub Json Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kss==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS Argument Error: Restart Jupyter Kernel Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-mecab-ko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 3.7.3 matches python-mecab-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import kss\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from mecab import MeCab\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AIHUB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIHUB 기계독해"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=89)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON File to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_json_txt_file_path_list\n",
    "from data_preprocessing import divide_source_file_list\n",
    "from extract_source_text import make_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_기계독해'+ '/**/*.json']\n",
    "txt_path_list =  [\"exploration/machine_reading_pro/AIHUB_machine_reading_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of file: 3\n"
     ]
    }
   ],
   "source": [
    "json_file_list, txt_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_index_df = pd.DataFrame(json_file_list, columns=['source_file_name'])\n",
    "source_file_index_df.to_excel(\"source_file_index/machine_reading_source_file_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "\n",
    "    source_file_by_batch_df = pd.DataFrame({'File':[0], 'Length of Source List':[0],\n",
    "                                        'The Number of TXT File':[0], \n",
    "                                        'Description':[0]})\n",
    "                                            \n",
    "    the_number_of_total_txt_file = 0\n",
    "    the_number_of_txt_file_list = []\n",
    "    \n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]   \n",
    "\n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file)\n",
    "\n",
    "        source_list = make_sources(one_json_sample)\n",
    "        \n",
    "        the_number_of_txt_file = ((len(source_list) // batch_size) + 1)\n",
    "\n",
    "        if len(source_list) >= batch_size:\n",
    "            source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                              len(source_list), the_number_of_txt_file, \"\"]\n",
    "            the_number_of_txt_file_list.append(the_number_of_txt_file)\n",
    "            the_number_of_total_txt_file  += the_number_of_txt_file\n",
    "\n",
    "        elif len(source_list) < batch_size:\n",
    "            source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                              len(source_list), the_number_of_txt_file,\n",
    "                                              \"not subject of batch. small source list.\"]\n",
    "            the_number_of_txt_file_list.append(1)\n",
    "            the_number_of_total_txt_file  += 1\n",
    "\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"The number of txt file:\", the_number_of_total_txt_file)\n",
    "\n",
    "    if 'rain' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading_train.xlsx\", index=False)\n",
    "    elif 'alid' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading_valid.xlsx\", index=False)\n",
    "    else:\n",
    "         source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading.xlsx\", index=False)\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_txt_file_list):\n",
    "  \n",
    "  \n",
    "  progress_length = sum(the_number_of_txt_file_list)\n",
    "  print(\"[Size]\")\n",
    "  print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "  print(\"\\n[Order]\")\n",
    "  pbar = tqdm(range(progress_length))\n",
    "  num = 0\n",
    "\n",
    "  for i in range(len(source_file_list)):\n",
    "\n",
    "    source_file = source_file_list[i]\n",
    "    \n",
    "    with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "      one_json_sample = json.load(one_json_file)\n",
    "\n",
    "    source_list = make_sources(one_json_sample)\n",
    "    \n",
    "    n = batch_size\n",
    "    source_batch_list = list(divide_source_file_list(source_list, n))\n",
    "      \n",
    "    for source_list in source_batch_list:\n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "          fp.write(\"\\n\".join(source_list)) \n",
    "        num += 1  \n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "  pbar.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1000\n",
      "The number of txt file: 104\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_txt_file, the_number_of_txt_file_list = count_number_of_txt_file_with_batch_list(json_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>File</th>\n",
       "      <th>Length of Source List</th>\n",
       "      <th>The Number of txt File</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_normal_squad_all.json  ~  AI...</td>\n",
       "      <td>47314</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_clue0529_squad_all.json  ~  ...</td>\n",
       "      <td>34500</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_noanswer_squad_all.json  ~  ...</td>\n",
       "      <td>20030</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               File  \\\n",
       "0           0  AIHUB_기계독해\\ko_nia_normal_squad_all.json  ~  AI...   \n",
       "1           1  AIHUB_기계독해\\ko_nia_clue0529_squad_all.json  ~  ...   \n",
       "2           2  AIHUB_기계독해\\ko_nia_noanswer_squad_all.json  ~  ...   \n",
       "\n",
       "   Length of Source List  The Number of txt File  Description  \n",
       "0                  47314                      48          NaN  \n",
       "1                  34500                      35          NaN  \n",
       "2                  20030                      21          NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_by_batch_df = pd.read_excel('source_file_by_batch/machine_reading.xlsx', engine='openpyxl')  \n",
    "source_file_by_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of preprocessing corpus: 104\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:20<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(json_file_list, txt_file_path_list,\n",
    "                batch_size, the_number_of_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_segmentation import preprocessing_text\n",
    "from data_preprocessing import make_pro_post_txt_file_path_list\n",
    "from data_preprocessing import merge_and_deduplicate_corpus_txt\n",
    "from reading_data import reading_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/machine_reading_pro/AIHUB_machine_reading_\" + \"*.txt\"\n",
    "pro_total_corpus_path_list, post_total_corpus_path_list = make_pro_post_txt_file_path_list(pro_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pro_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국청소년단체협의회와 여성가족부는 22일부터 28일까지 서울과 충북 괴산에서 '국제청소년포럼'을 연다고 21일 밝혔다. 한국 미국 캐나다 호주 등 전 세계 32개국 75여명의 대학생, 청소년들이 모여 전 세계적 현안문제에 대한 대안과 해결책을 모색하는 자리다. 이번 포럼의 주제는 '청소년과 뉴미디어'다. 스마트폰 SNS 태블릿PC 등 새로운 커뮤니케이션 매체인 '뉴미디어'에 대한 성찰과 문제점에 대해 토론한다. 기조강연을 시작으로 국가별 주제관련 사례발표, 그룹 토론 및 전체총회, '청소년선언문' 작성 및 채택 등 다양한 프로그램을 운영한다. 개회식은 22일 서울 방화동에 있는 국제청소년센터 국제회의장에서 한다. 전 세계 32개국 대학생ㆍ청소년 참가자와 전국의 청소년기관단체장과 청소년지도자 여성가족부 주한외교사절 등 100여명이 참석할 예정이다. 23일에는 유엔미래포럼 박영숙 대표가 '뉴미디어의 균형 있는 발전을 위한 청소년의 역할'에 대해 기조강연을 한다. 뉴미디어의 올바른 활용방안과 청소년문화의 형성에 대해 설명할 계획이다. 27일 폐회식에서는 '청소년선언문'을 채택한다. 선언문에는 전 세계적으로 뉴미디어의 바람직한 발전을 촉구하며 각국 청년들이 함께 실천할 수 있는 내용 등이 담길 예정이다. 한국청소년단체협의회는 포럼이 끝난 뒤 UN 등 국제기구와 참가자 각국 정부 등 국제사회에 선언문을 전달할 예정이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pro_coprus_file = pro_total_corpus_path_list[0]\n",
    "line_length = 1\n",
    "data_type = \"source\"\n",
    "\n",
    "reading_txt(pro_coprus_file, line_length, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국청소년단체협의회와 여성가족부는 22일부터 28일까지 서울과 충북 괴산에서  국제청소년포럼 을 연다고 21일 밝혔다.\n",
      "\n",
      "한국 미국 캐나다 호주 등 전 세계 32개국 75여명의 대학생, 청소년들이 모여 전 세계적 현안문제에 대한 대안과 해결책을 모색하는 자리다.\n",
      "\n",
      "스마트폰 SNS 태블릿PC 등 새로운 커뮤니케이션 매체인  뉴미디어 에 대한 성찰과 문제점에 대해 토론한다.\n",
      "\n",
      "기조강연을 시작으로 국가별 주제관련 사례발표, 그룹 토론 및 전체총회,  청소년선언문  작성 및 채택 등 다양한 프로그램을 운영한다.\n",
      "\n",
      "개회식은 22일 서울 방화동에 있는 국제청소년센터 국제회의장에서 한다.\n",
      "\n",
      "전 세계 32개국 대학생 청소년 참가자와 전국의 청소년기관단체장과 청소년지도자 여성가족부 주한외교사절 등 100여명이 참석할 예정이다.\n",
      "\n",
      "23일에는 유엔미래포럼 박영숙 대표가  뉴미디어의 균형 있는 발전을 위한 청소년의 역할 에 대해 기조강연을 한다.\n",
      "\n",
      "뉴미디어의 올바른 활용방안과 청소년문화의 형성에 대해 설명할 계획이다.\n",
      "\n",
      "선언문에는 전 세계적으로 뉴미디어의 바람직한 발전을 촉구하며 각국 청년들이 함께 실천할 수 있는 내용 등이 담길 예정이다.\n",
      "\n",
      "한국청소년단체협의회는 포럼이 끝난 뒤 UN 등 국제기구와 참가자 각국 정부 등 국제사회에 선언문을 전달할 예정이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pro_coprus_file = pro_total_corpus_path_list[0]\n",
    "line_length = 1\n",
    "data_type = \"preprocessing\"\n",
    "\n",
    "reading_txt(pro_coprus_file, line_length, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 10:42:24,945\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def ray_preprocessing_text(source, corpus_path):\n",
    "\n",
    "    preprocessing_sentence_list = preprocessing_text(source, corpus_path)\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "    \n",
    "    progress_length = len(pro_total_corpus_path_list)\n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "    print(\"\\n[Order]\")\n",
    "    pbar = tqdm(range(progress_length))\n",
    "    process_num = 10    \n",
    "\n",
    "    for pro, post in zip(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "\n",
    "        sentence_list = []\n",
    "\n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                futures = [ray_preprocessing_text.remote(lines[start_line:end_line][j], pro) for j in range(process_num)]\n",
    "                results = ray.get(futures)\n",
    "\n",
    "                if i == nested_lines_num - 2:\n",
    "                    futures = [ray_preprocessing_text.remote(lines[end_line:][j], pro) for j in range(len(lines) - end_line)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "\n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "\n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))\n",
    "\n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    pbar.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of preprocessing corpus: 104\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [1:07:27<16:03, 48.18s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 82%|████████▏ | 85/104 [1:08:02<15:12, 48.03s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 83%|████████▎ | 86/104 [1:08:38<14:22, 47.89s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 84%|████████▎ | 87/104 [1:09:15<13:31, 47.76s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 86%|████████▌ | 89/104 [1:10:27<11:52, 47.50s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 88%|████████▊ | 91/104 [1:11:45<10:15, 47.31s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 88%|████████▊ | 92/104 [1:12:23<09:26, 47.21s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 91%|█████████▏| 95/104 [1:14:04<07:01, 46.79s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 95%|█████████▌| 99/104 [1:16:20<03:51, 46.27s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 96%|█████████▌| 100/104 [1:16:56<03:04, 46.16s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=19244)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      "\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 97%|█████████▋| 101/104 [1:17:34<02:18, 46.09s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 98%|█████████▊| 102/104 [1:18:08<01:31, 45.97s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      "100%|██████████| 104/104 [1:18:44<00:00, 45.43s/it]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/machine_reading_post/AIHUB_machine_reading_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_machine_reading.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_machine_reading.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus_06",
   "language": "python",
   "name": "corpus_06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
