{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHub Json Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kss==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS Argument Error: Restart Jupyter Kernel Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-mecab-ko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 3.7.3 matches python-mecab-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import kss\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from mecab import MeCab\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AIHUB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "    \n",
    "    path_list = sorted(path_list, reverse=False)\n",
    "    path_list = sorted(path_list, key=len)\n",
    "    \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_source_file_list(l, n): \n",
    "    \n",
    "  for i in range(0, len(l), n): \n",
    "    yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    json_file_path = [glob(i, recursive = True) for i in path_list][0]\n",
    "    json_file_path = sorted_list(json_file_path)\n",
    "    \n",
    "    return json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "  train_json_file_path, valid_json_file_path = [glob(i, recursive = True) if 'rain' in i\n",
    "                                      else glob(i, recursive = True)\n",
    "                                      for i in path_list]\n",
    "\n",
    "  train_json_file_path = sorted_list(train_json_file_path)\n",
    "  valid_json_file_path = sorted_list(valid_json_file_path)\n",
    "    \n",
    "  return train_json_file_path, valid_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(json_file_path, folder_corpus_type_path):\n",
    "  \n",
    "  txt_file_path = [folder_corpus_type_path + str(i) + \".txt\"\n",
    "                              for i in range(len(json_file_path))]\n",
    "    \n",
    "  return txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    train_json_file_path, valid_json_file_path = train_valid_json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_train_json_file  = len(train_json_file_path)\n",
    "    the_number_of_valid_json_file  = len(valid_json_file_path)\n",
    "    the_number_of_json_file = the_number_of_train_json_file + the_number_of_valid_json_file\n",
    "    print(\"The number of train json file:\", the_number_of_train_json_file)\n",
    "    print(\"The number of valid json file:\", the_number_of_valid_json_file)\n",
    "    print(\"The number of json file:\", the_number_of_json_file)\n",
    "    \n",
    "    train_txt_file_path = txt_file_path_list(train_json_file_path, txt_path_list[0])\n",
    "    valid_txt_file_path = txt_file_path_list(valid_json_file_path, txt_path_list[1])\n",
    "\n",
    "    return train_json_file_path, valid_json_file_path, train_txt_file_path, valid_txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    json_file_path = json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(json_file_path) \n",
    "    print(\"The number of json file:\", the_number_of_json_file)\n",
    "    \n",
    "    txt_file_path = txt_file_path_list(json_file_path, txt_path_list[0])\n",
    "\n",
    "    return json_file_path, txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "        \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "        if source[0] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[-1] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[0] == '\"' and source[-1] == '\"':\n",
    "            del(source[0])\n",
    "            del(source[-1])\n",
    "        # 문장의 시작과 끝이 따옴표(\"\")이면 따옴표 제거\n",
    "        \n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence)    \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "                preprocessing_sentence_list.append(sentence)\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIHUB 기계독해"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=89)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON File to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_기계독해'+ '/**/*.json']\n",
    "txt_path_list =  [\"exploration/machine_reading_pro/AIHUB_machine_reading_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of file: 3\n"
     ]
    }
   ],
   "source": [
    "json_file_list, txt_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_index_df = pd.DataFrame(json_file_list, columns=['source_file_name'])\n",
    "source_file_index_df.to_excel(\"source_file_index/machine_reading_source_file_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_source_list(json_sample):\n",
    "\n",
    "    source_df = pd.DataFrame(json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    return source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "\n",
    "    source_file_by_batch_df = pd.DataFrame({'File':[0], 'Length of Source List':[0],\n",
    "                                        'The Number of TXT File':[0], \n",
    "                                        'Description':[0]})\n",
    "                                            \n",
    "    the_number_of_total_txt_file = 0\n",
    "    the_number_of_txt_file_list = []\n",
    "    \n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]   \n",
    "\n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file)\n",
    "\n",
    "        source_list = make_source_list(one_json_sample)\n",
    "        \n",
    "        the_number_of_txt_file = ((len(source_list) // batch_size) + 1)\n",
    "\n",
    "        if len(source_list) >= batch_size:\n",
    "            source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                              len(source_list), the_number_of_txt_file, \"\"]\n",
    "            the_number_of_txt_file_list.append(the_number_of_txt_file)\n",
    "            the_number_of_total_txt_file  += the_number_of_txt_file\n",
    "\n",
    "        elif len(source_list) < batch_size:\n",
    "            source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                              len(source_list), the_number_of_txt_file,\n",
    "                                              \"not subject of batch. small source list.\"]\n",
    "            the_number_of_txt_file_list.append(1)\n",
    "            the_number_of_total_txt_file  += 1\n",
    "\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"The number of txt file:\", the_number_of_total_txt_file)\n",
    "\n",
    "    if 'rain' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading_train.xlsx\", index=False)\n",
    "    elif 'alid' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading_valid.xlsx\", index=False)\n",
    "    else:\n",
    "         source_file_by_batch_df.to_excel(\"source_file_by_batch/machine_reading.xlsx\", index=False)\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_txt_file_list):\n",
    "  \n",
    "  \n",
    "  progress_length = sum(the_number_of_txt_file_list)\n",
    "  print(\"[Size]\")\n",
    "  print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "  print(\"\\n[Order]\")\n",
    "  pbar = tqdm(range(progress_length))\n",
    "  num = 0\n",
    "\n",
    "  for i in range(len(source_file_list)):\n",
    "\n",
    "    source_file = source_file_list[i]\n",
    "    \n",
    "    with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "      one_json_sample = json.load(one_json_file)\n",
    "\n",
    "    source_list = make_source_list(one_json_sample)\n",
    "    \n",
    "    n = batch_size\n",
    "    source_batch_list = list(divide_source_file_list(source_list, n))\n",
    "      \n",
    "    for source_list in source_batch_list:\n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "          fp.write(\"\\n\".join(source_list)) \n",
    "        num += 1  \n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "  pbar.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1000\n",
      "The number of txt file: 104\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_txt_file, the_number_of_txt_file_list = count_number_of_txt_file_with_batch_list(json_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>File</th>\n",
       "      <th>Length of Source List</th>\n",
       "      <th>The Number of txt File</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_normal_squad_all.json  ~  AI...</td>\n",
       "      <td>47314</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_clue0529_squad_all.json  ~  ...</td>\n",
       "      <td>34500</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AIHUB_기계독해\\ko_nia_noanswer_squad_all.json  ~  ...</td>\n",
       "      <td>20030</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               File  \\\n",
       "0           0  AIHUB_기계독해\\ko_nia_normal_squad_all.json  ~  AI...   \n",
       "1           1  AIHUB_기계독해\\ko_nia_clue0529_squad_all.json  ~  ...   \n",
       "2           2  AIHUB_기계독해\\ko_nia_noanswer_squad_all.json  ~  ...   \n",
       "\n",
       "   Length of Source List  The Number of txt File  Description  \n",
       "0                  47314                      48          NaN  \n",
       "1                  34500                      35          NaN  \n",
       "2                  20030                      21          NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_by_batch_df = pd.read_excel('source_file_by_batch/machine_reading.xlsx', engine='openpyxl')  \n",
    "source_file_by_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of preprocessing corpus: 104\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:20<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(json_file_list, txt_file_path_list,\n",
    "                batch_size, the_number_of_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_path_list):\n",
    "   \n",
    "  post_corpus_path_list = [corpus_file.replace(\"pro\", \"post\")\n",
    "                      for corpus_file in corpus_path_list]\n",
    "\n",
    "  return post_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    pro_total_corpus_path_list = glob(pro_corpus_path)\n",
    "    pro_total_corpus_path_list = sorted_list(pro_total_corpus_path_list)\n",
    "    post_total_corpus_path_list = post_txt_file_path_list(pro_total_corpus_path_list)\n",
    "\n",
    "    return pro_total_corpus_path_list, post_total_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/machine_reading_pro/AIHUB_machine_reading_\" + \"*.txt\"\n",
    "pro_total_corpus_path_list, post_total_corpus_path_list = make_pro_post_txt_file_path_list(pro_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pro_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국청소년단체협의회와 여성가족부는 22일부터 28일까지 서울과 충북 괴산에서 '국제청소년포럼'을 연다고 21일 밝혔다. 한국 미국 캐나다 호주 등 전 세계 32개국 75여명의 대학생, 청소년들이 모여 전 세계적 현안문제에 대한 대안과 해결책을 모색하는 자리다. 이번 포럼의 주제는 '청소년과 뉴미디어'다. 스마트폰 SNS 태블릿PC 등 새로운 커뮤니케이션 매체인 '뉴미디어'에 대한 성찰과 문제점에 대해 토론한다. 기조강연을 시작으로 국가별 주제관련 사례발표, 그룹 토론 및 전체총회, '청소년선언문' 작성 및 채택 등 다양한 프로그램을 운영한다. 개회식은 22일 서울 방화동에 있는 국제청소년센터 국제회의장에서 한다. 전 세계 32개국 대학생ㆍ청소년 참가자와 전국의 청소년기관단체장과 청소년지도자 여성가족부 주한외교사절 등 100여명이 참석할 예정이다. 23일에는 유엔미래포럼 박영숙 대표가 '뉴미디어의 균형 있는 발전을 위한 청소년의 역할'에 대해 기조강연을 한다. 뉴미디어의 올바른 활용방안과 청소년문화의 형성에 대해 설명할 계획이다. 27일 폐회식에서는 '청소년선언문'을 채택한다. 선언문에는 전 세계적으로 뉴미디어의 바람직한 발전을 촉구하며 각국 청년들이 함께 실천할 수 있는 내용 등이 담길 예정이다. 한국청소년단체협의회는 포럼이 끝난 뒤 UN 등 국제기구와 참가자 각국 정부 등 국제사회에 선언문을 전달할 예정이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines() \n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:\n",
    "           line_list.append(line)\n",
    "        \n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국청소년단체협의회와 여성가족부는 22일부터 28일까지 서울과 충북 괴산에서  국제청소년포럼 을 연다고 21일 밝혔다.\n",
      "\n",
      "한국 미국 캐나다 호주 등 전 세계 32개국 75여명의 대학생, 청소년들이 모여 전 세계적 현안문제에 대한 대안과 해결책을 모색하는 자리다.\n",
      "\n",
      "스마트폰 SNS 태블릿PC 등 새로운 커뮤니케이션 매체인  뉴미디어 에 대한 성찰과 문제점에 대해 토론한다.\n",
      "\n",
      "기조강연을 시작으로 국가별 주제관련 사례발표, 그룹 토론 및 전체총회,  청소년선언문  작성 및 채택 등 다양한 프로그램을 운영한다.\n",
      "\n",
      "개회식은 22일 서울 방화동에 있는 국제청소년센터 국제회의장에서 한다.\n",
      "\n",
      "전 세계 32개국 대학생 청소년 참가자와 전국의 청소년기관단체장과 청소년지도자 여성가족부 주한외교사절 등 100여명이 참석할 예정이다.\n",
      "\n",
      "23일에는 유엔미래포럼 박영숙 대표가  뉴미디어의 균형 있는 발전을 위한 청소년의 역할 에 대해 기조강연을 한다.\n",
      "\n",
      "뉴미디어의 올바른 활용방안과 청소년문화의 형성에 대해 설명할 계획이다.\n",
      "\n",
      "선언문에는 전 세계적으로 뉴미디어의 바람직한 발전을 촉구하며 각국 청년들이 함께 실천할 수 있는 내용 등이 담길 예정이다.\n",
      "\n",
      "한국청소년단체협의회는 포럼이 끝난 뒤 UN 등 국제기구와 참가자 각국 정부 등 국제사회에 선언문을 전달할 예정이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:  \n",
    "            sentences = formal_preprocessing_text(line)\n",
    "            for sentence in sentences:\n",
    "                line_list.append(sentence) \n",
    "            \n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 10:42:24,945\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "        \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "\n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "            if source[0] == '\"':\n",
    "                del(source[0])\n",
    "            elif source[-1] == '\"':\n",
    "                del(source[0])\n",
    "            elif source[0] == '\"' and source[-1] == '\"':\n",
    "                del(source[0])\n",
    "                del(source[-1])\n",
    "            # 문장의 시작과 끝이 따옴표(\"\")이면 따옴표 제거\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence)    \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "                preprocessing_sentence_list.append(sentence)\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "    \n",
    "    progress_length = len(pro_total_corpus_path_list)\n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "    print(\"\\n[Order]\")\n",
    "    pbar = tqdm(range(progress_length))\n",
    "    process_num = 10    \n",
    "\n",
    "    for pro, post in zip(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "\n",
    "        sentence_list = []\n",
    "\n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                results = ray.get(futures)\n",
    "\n",
    "                if i == nested_lines_num - 2:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "\n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "\n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))\n",
    "\n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    pbar.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of preprocessing corpus: 104\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [1:07:27<16:03, 48.18s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 82%|████████▏ | 85/104 [1:08:02<15:12, 48.03s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 83%|████████▎ | 86/104 [1:08:38<14:22, 47.89s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 84%|████████▎ | 87/104 [1:09:15<13:31, 47.76s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 86%|████████▌ | 89/104 [1:10:27<11:52, 47.50s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 88%|████████▊ | 91/104 [1:11:45<10:15, 47.31s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 88%|████████▊ | 92/104 [1:12:23<09:26, 47.21s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 91%|█████████▏| 95/104 [1:14:04<07:01, 46.79s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 95%|█████████▌| 99/104 [1:16:20<03:51, 46.27s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=2288)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 96%|█████████▌| 100/104 [1:16:56<03:04, 46.16s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=19244)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      "\u001b[2m\u001b[36m(formal_preprocessing_text pid=18688)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 97%|█████████▋| 101/104 [1:17:34<02:18, 46.09s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      " 98%|█████████▊| 102/104 [1:18:08<01:31, 45.97s/it]\u001b[2m\u001b[36m(formal_preprocessing_text pid=6932)\u001b[0m [Korean Sentence Splitter]: Too long text! turn off quotes calibration!\n",
      "100%|██████████| 104/104 [1:18:44<00:00, 45.43s/it]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    corpus_list = glob(preprocessing_corpus_path)\n",
    "    corpus_list = sorted_list(corpus_list)\n",
    "    \n",
    "    with open(merge_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for corpus in corpus_list:\n",
    "            with open(corpus, encoding='utf-8') as text:\n",
    "                for line in text:\n",
    "                    f.write(line)\n",
    "                    \n",
    "    with open(deduplicate_corpus_path, 'w', encoding='utf-8') as f1:\n",
    "        with open(merge_corpus_path, encoding='utf-8') as f2:\n",
    "            lines = f2.read().splitlines()\n",
    "            single_sentence_dict = dict.fromkeys(lines)\n",
    "            single_sentence_list = list(single_sentence_dict)\n",
    "            f1.write(\"\\n\".join(single_sentence_list))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/machine_reading_post/AIHUB_machine_reading_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_machine_reading.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_machine_reading.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus_06",
   "language": "python",
   "name": "corpus_06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
