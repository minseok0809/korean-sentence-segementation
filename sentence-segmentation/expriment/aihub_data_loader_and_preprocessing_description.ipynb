{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB Data Loader & Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>목차 </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> 1) 데이터 로더 <b/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 순서도\n",
    "<br><JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장>에 관한 순서도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> 2) 데이터 로더의 분류 <b/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) 원천 데이터의 Json 파일명\n",
    "<br>2.2) 원천 데이터의 추출한 텍스트 리스트의 크기\n",
    "<br>2.3) Key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> 3) 데이터 로더 및 전처리</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) 전문분야 말뭉치\n",
    "<br>3.2) 웹데이터 기반 한국어 말뭉치 데이터 \n",
    "<br>3.3) 산업정보 연계 주요국 특허 영-한 데이터 \n",
    "<br>3.4) 논문자료 요약\n",
    "<br>3.5) 요약문 및 레포트 생성 데이터\n",
    "<br>3.6) 문서요약 텍스트\n",
    "<br>3.7) 특허 분야 자동분류 데이터 \n",
    "<br>3.8) 뉴스 기사 기계독해 데이터\n",
    "<br>3.9) 행정 문서 대상 기계독해 데이터\n",
    "<br>3.10) 기계독해 \n",
    "<br>3.11) 도서자료 요약\n",
    "<br>3.12) 대규모 구매도서 기반 한국어 말뭉치 데이터 \n",
    "<br>3.13) 도서자료 기계독해\n",
    "<br>3.14) 법률 규정 (판결서 약관 등) 텍스트 분석 데이터\n",
    "<br>3.15) 일반상식"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 1) 데이터 로더 <br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 순서도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"https://github.com/minseok0809/text-preprocessing/assets/97289420/c1bcfdf1-edac-47b1-b1f0-b63cd52b2ae3\" width=\"500px\" style= \"padding-right:100px\"> \n",
    "<img src=\"https://github.com/minseok0809/text-preprocessing/assets/97289420/fdca2335-2655-4052-a630-df7f4fcafa24\" width=\"500px\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[데이터 로드]</b>\n",
    "<br>원천 데이터 JSON에서 추출한 텍스트를 TXT 파일에 그대로 저장\n",
    "<br>그 TXT 파일을 읽어와서 전처리 작업 수행하고 TXT 파일로 하나씩 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[파라미터]</b>\n",
    "<br>하나의 TXT 파일에 원천 데이터 텍스트를 얼마나 많이 저장할지는 Batch Size가 결정한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[목적]</b>\n",
    "<br>여러 TXT파일에 원천 데이터 텍스트를 분배하여 전처리 작업을 수행하면 여러 장점이 있다.\n",
    "<br>- 현재까지 전처리한 TXT 파일 개수가 작업 진행 상황을 알려주는 지표이다.\n",
    "<br>- 작업 도중에 실행 오류가 떠도 다음 TXT 파일 순서부터 전처리 작업을 수행할 수 있다.\n",
    "<br>- TXT 파일의 텍스트 크기만큼 메모리를 차지하면서 할당하는 메모리 양이 줄어든다. \n",
    "<br>- 전처리 작업 이전과 이후의 결과물을 여러 개의 용량 작은 TXT 파일을 열어보면서 비교할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장>에 관한 순서도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/minseok0809/text-preprocessing/assets/97289420/0f9b9e4a-9319-4066-990b-be44259e0616\" width=\"1000px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터에서 추출한 텍스트 리스트 크기를 Batch Size에 맞춤\n",
    "<br>Batch Size 이상인 텍스트 리스트를 TXT 파일에 한 줄씩 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분 데이터셋의 Batch Size를 len(list) = 1000으로 설정한다. \n",
    "<br>대부분 데이터셋의 JSON 파일은 수백개에서 수십만개의 텍스트 리스트를 가지고 있기 때문에 len(list) = 1000를 Batch Size의 기준으로 잡아 리스트를 분할한다.\n",
    "<br><br>데이터셋 <논문자료 요약>에서는 다른 데이터셋과 비교했을 때 상당히 긴 문장이 있다.\n",
    "<br>문장의 길이가 길수록 차지하는 메모리 용량이 커지기 때문에 <논문자료 요약>에서는 Batch Size를 len(list) = 100으로 설정한다.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 2) 데이터 로더의 분류</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Hub의 데이터 로더는 다음의 기준에 의해서 분류된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>1) 원천 데이터의 Json 파일명\n",
    "<br>2) 원천 데이터의 추출한 텍스트 리스트의 크기\n",
    "<br>3) Key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) 원천 데이터의 Json 파일명"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원천 데이터의 Json 파일명에 따라 파일 경로 리스트가 달라짐"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/minseok0809/text-preprocessing/assets/97289420/cb916cc3-15a1-4c7a-9173-335ce5d5b8cb\" width=\"1600px\"><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) 원천 데이터의 추출한 텍스트 리스트의 크기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원천 데이터에서 추출한 텍스트 리스트 크기를 Batch Size에 맞춤"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/minseok0809/text-preprocessing/assets/97289420/f3614b77-bc16-48cc-9ca1-c7a61c2dc78c\" width=\"1400px\"><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<요약문 및 레포트 생성 데이터>, <법률 규정 (판결서 약관 등) 텍스트 분석 데이터>, <도서자료 요약>은 하나의 JSON 파일이 차지하는 텍스트 리스트의 크기가 30 이하로 작기 때문에\n",
    "<br>len(list) >= Batch Size일 때까지 여러 개의 JSON 파일에서 리스트를 병합한다.\n",
    "<br>대부분 데이터셋의 JSON 파일은 수백개에서 수십만개의 텍스트 리스트를 가지고 있기 때문에 Batch Size를 기준으로 리스트를 분할한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3)  Key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tbody><tr><td>데이터</td><td>Key</td><td> 비고</td></tr>\n",
    "<tr><td>전문분야 말뭉치</td><td>\n",
    "<br>논문_## / 특허_z_##\n",
    ": “data” - “text”\n",
    "\n",
    "<br>특허_숫자_##\n",
    ": “data” - “sentence”\n",
    "\n",
    "<br>법령_## / 판례_##\n",
    ": “data” - “sentence” - “text”</td><td>\n",
    "<br>특허_z_## : \"attr\" 키의 value가 \"abs\" 인 경우만 남긴다.\n",
    "\n",
    "<br>특허_숫자_## : \"attr\" 키의 value가 \"초록\" 인 경우만 남긴다.</td></tr>\n",
    "\n",
    "<tr><td>웹데이터 기반 한국어 말뭉치 데이터</td><td>'SJML' - 'text' - 'content'</td><td></td></tr>\n",
    "\n",
    "<tr><td>산업정보 연계 주요국 특허 영-한 데이터</td><td>'labeled_data' - 'astrt_cont_kor'</td><td></td></tr>\n",
    "\n",
    "<tr><td>논문자료 요약</td><td>\n",
    "<br>논문요약_ ##  /  특허섹션_##\n",
    "<br>1) 'data' - 'summary_entire' -  'orginal_text'\n",
    "<br>2) 'data' - 'summary_section' -  'orginal_text'\n",
    "\n",
    "<br>특허전체_##\n",
    "'data' - 'summary_section' -  'orginal_text'</td><td></td></tr>\n",
    "\n",
    "<tr><td>요약문 및 레포트 생성 데이터</td><td>'Meta(Refine)' – 'passage'</td><td></td></tr>\n",
    "\n",
    "<tr><td>문서요약 텍스트</td><td>'document' – 'text'</td><td>\n",
    "Key \"text\"의 \"sentence\"를 하나의 통합한\n",
    "<br>새로운 변수 'source_sentence' 만든 다음 공통 전처리 \n",
    "</td></tr>\n",
    "\n",
    "<tr><td>특허 분야 자동분류 데이터</td><td>'dataset' – 'abstract'</td><td></td></tr>\n",
    "\n",
    "<tr><td>뉴스 기사 기계독해 데이터</td><td>'data' – 'paragraph' – 'context'</td><td></td></tr>\n",
    "\n",
    "<tr><td>행정 문서 대상 기계독해 데이터</td><td>'data' – 'paragraph' – 'context'</td><td></td></tr>\n",
    "\n",
    "<tr><td>기계독해</td><td>'data' – 'paragraph' – 'context'</td><td></td></tr>\n",
    "\n",
    "<tr><td>도서자료 요약</td><td>\n",
    "<br>1) 'passage'\n",
    "<br>2) 'summary'\n",
    "</td><td></td></tr>\n",
    "\n",
    "<tr><td>대규모 구매도서 기반 한국어 말뭉치 데이터</td><td>'paragraphs' – 'sentences'</td><td>\n",
    "라벨링 데이터 사용\n",
    "<br>BOOK_CORPUS_####_INFO.json 파일 제외\n",
    "</td></tr>\n",
    "\n",
    "<tr><td>도서자료 기계독해</td><td>'data' – 'paragraph' – 'context'</td><td></td></tr>\n",
    "\n",
    "<tr><td>법률 규정 (판결서 약관 등) 텍스트 분석 데이터</td><td>\n",
    "<br>판결문\n",
    "<br>1) 'disposal' - 'disposalcontent'\n",
    "<br>2) 'mentionedItems' - 'rqestObjet'\n",
    "<br>3) 'assrs' - 'acusrAssrs'\n",
    "<br>4) 'assrs' - 'dedatAssrs'\n",
    "<br>5) 'facts' - 'bsisFacts'\n",
    "<br>6) 'dcss' - 'courtDcss'\n",
    "<br>7) 'close' - 'cnclsns'\n",
    "\n",
    "<br>약관 – 01.유리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) 'comProvision'\n",
    "\n",
    "<br>약관 – 02.불리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) ''illdcssBasiss'\n",
    "<br>3) 'relateLaword'</td><td>\n",
    "<br>라벨링 데이터</td></tr>\n",
    "\n",
    "<tr><td>일반상식</td><td>\n",
    "<br>ko_wiki_v1_squad.json\n",
    "<br>'data' – 'paragraph' – 'context'\n",
    "\n",
    "<br>edited_####.json\n",
    "<br>'sentence' - 'text'\"</td><td></td></tr>\n",
    "\n",
    "</tbody></table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 3) 데이터 로더 및 전처리</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>라이브러리 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kss==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS Argument Error: Restart Jupyter Kernel Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-mecab-ko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 3.7.3 matches python-mecab-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import kss\n",
    "import ray\n",
    "import json\n",
    "from mecab import MeCab\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>KSS(한국어 문장 분리기)</b>\n",
    "\n",
    "<br>Github: https://github.com/hyunwoongko/kss\n",
    "\n",
    "<br>오픈소스 라이브러리 버전: 3.7.3\n",
    "\n",
    "<br>라이센스: 3-Clause BSD License(BSD-3-Clause)\n",
    "\n",
    "<br>작동 방식:\n",
    "<br>KSS의 함수 split_sentences의 argument에는 ‘use_heuristic (bool)’, ‘backend (str)’ 등이 있다.\n",
    "<br>‘use_heuristic (bool)'에서 False를 설정하면, 문장부호를 기준으로 문장 분리를 한다(Punctuation-only segmentation). 문법을 준수하는 공식적인 텍스트(wiki, news, essays)에 맞는 문장분리 방식이다.\n",
    "<br>C++가 작동할 수 있는 작업환경이면, 형태소분석기 Mecab을 KSS의 ‘backend (str)’로  사용하여 KSS 라이브러리 실행 속도를 높인다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) 전문분야 말뭉치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=110)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>논문_## / 특허_z_##\n",
    "<br>: “data” - “text”\n",
    "<br><br>특허_숫자_##\n",
    "<br>: “data” - “sentence”\n",
    "<br><br>법령_## / 판례_##\n",
    "<br>: “data” - “sentence” - “text” "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "    path_list = sorted(path_list, reverse=False)\n",
    "    path_list = sorted(path_list, key=len)\n",
    "    \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "  train_file_path, valid_file_path = [glob(i, recursive = True) if 'rain' in i\n",
    "                                      else glob(i, recursive = True)\n",
    "                                      for i in path_list]\n",
    "\n",
    "  train_file_path = sorted_list(train_file_path)\n",
    "  valid_file_path = sorted_list(valid_file_path)\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  text_file_path_list = [folder_corpus_type_path + str(i) + \".txt\"\n",
    "                              for i in range(len(source_file_nested_list))]\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    train_file_path, valid_file_path = train_valid_json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(train_file_path) + len(valid_file_path)\n",
    "    print(\"The number of file:\", the_number_of_json_file)\n",
    "    \n",
    "    train_text_file_path_list = txt_file_path_list(train_file_path, txt_path_list[0])\n",
    "    valid_text_file_path_list = txt_file_path_list(valid_file_path, txt_path_list[1])\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_전문분야 말뭉치/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_전문분야 말뭉치/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/professional_corpus_pro/AIHUB_professional_corpus_train_\", \n",
    "                 \"exploration/professional_corpus_pro/AIHUB_professional_corpus_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    the_number_of_total_txt_file = 0\n",
    "    the_number_of_txt_file_list = []\n",
    "    \n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]        \n",
    "\n",
    "            \n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file) \n",
    "\n",
    "        if '논문_' in source_file:\n",
    "            source_list = list(pd.DataFrame(one_json_sample['data'])['text'])\n",
    "            \n",
    "        elif '특허_z' in source_file:\n",
    "            source_df = pd.DataFrame(one_json_sample['data'])\n",
    "            source_list = list(source_df[source_df['attr'] == 'abs']['text'])\n",
    "\n",
    "        elif '특허_0' in source_file or '특허_1' in source_file:\n",
    "            source_df = pd.DataFrame(one_json_sample['data'])\n",
    "            source_dict = dict(source_df[source_df['attr'] == '초록']['sentence'].explode())\n",
    "            source_json = pd.json_normalize(source_dict)\n",
    "            source_list = list(source_json.filter(regex='text').values[0])\n",
    "\n",
    "        elif '법령' in source_file or '판례' in source_file:\n",
    "            source_df = pd.DataFrame(one_json_sample['data'])\n",
    "            source_dict = dict(source_df['sentence'].explode())\n",
    "            source_json = pd.json_normalize(source_dict)\n",
    "            source_list = list(source_json.filter(regex='text').values[0])\n",
    "\n",
    "        the_number_of_txt_file = ((len(source_list) // batch_size) + 1)\n",
    "\n",
    "        if len(source_list) >= batch_size:\n",
    "            print(\"File:\", source_file)    \n",
    "            print(\"Index:\", i, \"  \", \"Length of Source List:\", len(source_list), \\\n",
    "                \"  \", \"The number of txt file:\", the_number_of_txt_file, \"\\n\")\n",
    "            the_number_of_txt_file_list.append(the_number_of_txt_file)\n",
    "            the_number_of_total_txt_file  += the_number_of_txt_file\n",
    "        else:\n",
    "            the_number_of_total_txt_file  += 1\n",
    "            the_number_of_txt_file_list.append(1)\n",
    "            print(\"[For Example]\")\n",
    "            print(\"This is not subject of batch. It's small source list.\")                            \n",
    "            print(\"File:\", source_file)\n",
    "            print(\"Length of Source List:\", len(source_list), \n",
    "                    \"  \", \"The number of txt file:\", 1, \"\\n\") \n",
    "\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"The number of txt file:\", the_number_of_total_txt_file)\n",
    "    \n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_total_txt_file_list):\n",
    "\n",
    "  print(\"[Size]\")\n",
    "  print(\"The number of preprocessing corpus: \" + str(sum(the_number_of_total_txt_file_list)))\n",
    "  print(\"\\n[Order]\")\n",
    "  num = 0\n",
    "  \n",
    "  for i in range(len(source_file_list)):\n",
    "\n",
    "    source_file = source_file_list[i]\n",
    "\n",
    "    with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "      one_json_sample = json.load(one_json_file)\n",
    "\n",
    "    if '논문_' in source_file:\n",
    "      source_list = list(pd.DataFrame(one_json_sample['data'])['text'])\n",
    "\n",
    "    elif '특허_z' in source_file:\n",
    "      source_df = pd.DataFrame(one_json_sample['data'])\n",
    "      source_list = list(source_df[source_df['attr'] == 'abs']['text'])\n",
    "\n",
    "    elif '특허_0' in source_file or '특허_1' in source_file:\n",
    "        source_df = pd.DataFrame(one_json_sample['data'])\n",
    "        source_dict = dict(source_df[source_df['attr'] == '초록']['sentence'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='text').values[0])\n",
    "\n",
    "    elif '법령' in source_file or '판례' in source_file:\n",
    "        source_df = pd.DataFrame(one_json_sample['data'])\n",
    "        source_dict = dict(source_df['sentence'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='text').values[0])\n",
    "        \n",
    "    n = batch_size\n",
    "    source_batch_list = list(divide_source_file_list(source_list, n))\n",
    "      \n",
    "    for source_list in source_batch_list:\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "      \n",
    "        if '법령' in source_file or '판례' in source_file:\n",
    "\n",
    "            with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \"_legal_text.txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "                fp.write(\"\\n\".join(source_list))   \n",
    "  \n",
    "        else:\n",
    "\n",
    "            with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "                fp.write(\"\\n\".join(source_list))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>논문_## / 특허_z_##\n",
    "<br>: “data” - “text”\n",
    "<br><br>특허_숫자_##\n",
    "<br>: “data” - “sentence”\n",
    "<br><br>법령_## / 판례_##\n",
    "<br>: “data” - “sentence” - “text” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def professional_corpus_post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  post_corpus_list = [corpus_file.replace(\"professional_corpus_pro\", \"professional_corpus_post\")\n",
    "                      for corpus_file in corpus_list]\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    pro_total_corpus_list = glob(pro_corpus_path)\n",
    "    pro_total_corpus_list = sorted_list(pro_total_corpus_list)\n",
    "    post_total_corpus_list = professional_corpus_post_txt_file_path_list(pro_total_corpus_list)\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/professional_corpus_pro/AIHUB_professional_corpus_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list\n",
    "\n",
    "@ray.remote\n",
    "def legal_preprocessing_text(source):    \n",
    "\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬처리 라이브러리 ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(len(pro_total_corpus_list)))\n",
    "    print(\"\\n[Order]\")\n",
    "    num = 0\n",
    "    process_num = 10    \n",
    "\n",
    "    for pro, post in zip(pro_total_corpus_list, post_total_corpus_list):\n",
    "        \n",
    "        sentence_list = []\n",
    "        \n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                \n",
    "                if 'legal_text' in pro:\n",
    "                    futures = [legal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [legal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "                        \n",
    "                else:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "                    \n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "        \n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "        \n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "            \n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로세서 개수만큼 코퍼스를 분리하여 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    corpus_list = glob(preprocessing_corpus_path)\n",
    "    corpus_list = sorted_list(corpus_list)\n",
    "    \n",
    "    with open(merge_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for corpus in corpus_list:\n",
    "            with open(corpus, encoding='utf-8') as text:\n",
    "                for line in text:\n",
    "                    f.write(line)\n",
    "                    \n",
    "    with open(deduplicate_corpus_path, 'w', encoding='utf-8') as f1:\n",
    "        with open(merge_corpus_path, encoding='utf-8') as f2:\n",
    "            lines = f2.read().splitlines()\n",
    "            single_sentence_dict = dict.fromkeys(lines)\n",
    "            single_sentence_list = list(single_sentence_dict)\n",
    "            f1.write(\"\\n\".join(single_sentence_list))                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/professional_corpus_source_post/AIHUB_professional_corpus_source_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_professional_corpus_source.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_professional_corpus_source.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) 웹데이터 기반 한국어 말뭉치 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'SJML' - 'text' - 'content'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_json_txt_file_path_list(json_folder_list, topic_name_list, txt_path_list):\n",
    "    \n",
    "    json_path_list = []\n",
    "    the_number_of_file = 0\n",
    "    the_numer_of_txt_file = 0\n",
    "    \n",
    "    for json_folder in json_folder_list:\n",
    "        for topic_name in topic_name_list:\n",
    "            json_path_list.append(json_folder + topic_name + '/**/*.json')\n",
    "\n",
    "    for i in range(len(json_path_list)):\n",
    "        \n",
    "        json_path = json_path_list[i]\n",
    "        \n",
    "        if 'train'[1:] in json_path:\n",
    "            train_file_path = glob(json_path, recursive = True) \n",
    "            the_number_of_file += len(train_file_path)\n",
    "            \n",
    "            if i < 9:\n",
    "                train_file_list_variable = \"train_file_list_\" + \"0\" + str(i + 1)\n",
    "                txt_file_path_list_variable = \"train_txt_file_path_list_\" + \"0\" + str(i + 1)\n",
    "                \n",
    "            elif i >= 9:\n",
    "                train_file_list_variable = \"train_file_list_\" + str(i + 1)\n",
    "                txt_file_path_list_variable = \"train_txt_file_path_list_\" + str(i + 1)\n",
    "                \n",
    "            globals()[train_file_list_variable] = train_file_path\n",
    "            globals()[txt_file_path_list_variable] = txt_file_path_list(train_file_path, \n",
    "                                                                        txt_path_list[0] + topic_name_list[i] + \"_train_\")   \n",
    "            \n",
    "        elif 'valid'[1:] in json_path:\n",
    "            valid_file_path = glob(json_path, recursive = True) \n",
    "            the_number_of_file += len(valid_file_path)\n",
    "            \n",
    "            if i < 9:\n",
    "                valid_file_list_variable = \"valid_file_list_\" + \"0\" + str(i + 1)\n",
    "                txt_file_path_list_variable = \"valid_txt_file_path_list_\" + \"0\" + str(i + 1)\n",
    "                \n",
    "            elif i >= 9:\n",
    "                valid_file_list_variable = \"valid_file_list_\" + str(i + 1)\n",
    "                txt_file_path_list_variable = \"valid_txt_file_path_list_\" + str(i + 1)\n",
    "                \n",
    "            globals()[valid_file_list_variable] = valid_file_nested_list\n",
    "            globals()[txt_file_path_list_variable] = txt_file_path_list(valid_file_path, \n",
    "                                                                        txt_path_list[1] + topic_name_list[i - (len(json_path_list)) // 2] + \"_valid_\")  \n",
    "\n",
    "    print(\"The number of file: \", the_number_of_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder_list = ['AIHUB_웹데이터 기반 한국어 말뭉치 데이터/Training/원천데이터/TS1/',\n",
    "             'AIHUB_웹데이터 기반 한국어 말뭉치 데이터/Validation/원천데이터/VS1/']\n",
    "\n",
    "topic_name_list = ['01_IT_과학', '02_건강', '03_경제', '04_교육', '05_국제', '06_라이프스타일',\n",
    "                   '07_문화', '08_사건사고', '09_사회일반', '10_산업', '11_스포츠', '12_여성복지',\n",
    "                   '13_여행레저', '14_연예', '15_정치', '16_지역', '17_취미']\n",
    "\n",
    "txt_path_list = [\"exploration/web_data_based_korean_corpus_data_pro/AIHUB_web_data_based_korean_corpus_data_train_\", \n",
    "                 \"exploration/web_data_based_korean_corpus_data_pro/AIHUB_web_data_based_korean_corpus_data_valid_\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_topic_json_txt_file_path_list(json_folder_list, topic_name_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_list = list(pd.DataFrame(one_json_sample['SJML']['text'])['content'])\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_total_txt_file_list):\n",
    "\n",
    "\n",
    "   ... \n",
    "\n",
    "   source_list = list(pd.DataFrame(one_json_sample['SJML']['text'])['content'])\n",
    "\n",
    "   ... \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'SJML' - 'text' - 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file_01, the_number_of_train_txt_file_list_01 = count_number_of_txt_file_with_batch_list(train_file_list_01, batch_size)\n",
    "\n",
    "...\n",
    "\n",
    "the_number_of_train_txt_file_17, the_number_of_train_txt_file_list_17 = count_number_of_txt_file_with_batch_list(train_file_list_17, batch_size)\n",
    "\n",
    "the_number_of_valid_txt_file_01, the_number_of_valid_txt_file_list_01 = count_number_of_txt_file_with_batch_listvalid_file_list_01, batch_size)\n",
    "\n",
    "...\n",
    "\n",
    "the_number_of_valid_txt_file_17, the_number_of_valid_txt_file_list_17 = count_number_of_txt_file_with_batch_list(valid_file_list_17, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list_01, train_text_file_path_list_01, batch_size, the_number_of_train_txt_file_list_01)\n",
    "...\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list_17, train_text_file_path_list_17, batch_size, the_number_of_train_txt_file_list_17)\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list_01, valid_text_file_path_list_01, batch_size, the_number_of_valid_txt_file_list_01)\n",
    "...\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list_17, valid_text_file_path_list_17, batch_size, the_number_of_valid_txt_file_list_17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_pro_post_txt_file_name_list(pro_corpus_path, topic_name_list):\n",
    "    \n",
    "\n",
    "    for i in range(len(topic_name_list)):\n",
    " \n",
    "        pro_total_corpus_list = sorted_list(glob(pro_corpus_path + topic_name_list[i] + \"_train_\" + \"*.txt\"))\n",
    "        post_total_corpus_list = post_txt_file_name_list(pro_total_corpus_list)\n",
    "        \n",
    "        if i < 9:\n",
    "            pro_total_corpus_list_variable = \"pro_total_corpus_list_\" + \"train_\" + \"0\" + str(i + 1)\n",
    "            post_total_corpus_list_variable = \"post_total_corpus_list_\" + \"train_\" + \"0\" + str(i + 1)\n",
    "        \n",
    "        elif i >= 9:\n",
    "            pro_total_corpus_list_variable = \"pro_total_corpus_list_\" + \"train_\" + str(i + 1)\n",
    "            post_total_corpus_list_variable = \"post_total_corpus_list_\" + \"train_\" + str(i + 1)\n",
    "            \n",
    "        globals()[pro_total_corpus_list_variable] = pro_total_corpus_list\n",
    "        globals()[post_total_corpus_list_variable] = post_total_corpus_list\n",
    "        \n",
    "        \n",
    "        pro_total_corpus_list = sorted_list(glob(pro_corpus_path + topic_name_list[i] + \"_valid_\" + \"*.txt\"))\n",
    "        post_total_corpus_list = post_txt_file_name_list(pro_total_corpus_list)\n",
    "        \n",
    "        if i < 9:\n",
    "            pro_total_corpus_list_variable = \"pro_total_corpus_list_\" + \"valid_\" + \"0\" + str(i + 1)\n",
    "            post_total_corpus_list_variable = \"post_total_corpus_list_\" + \"valid_\" + \"0\" + str(i + 1)\n",
    "        \n",
    "        elif i >= 9:\n",
    "            pro_total_corpus_list_variable = \"pro_total_corpus_list_\" + \"valid_\" + str(i + 1)\n",
    "            post_total_corpus_list_variable = \"post_total_corpus_list_\" + \"valid_\" + str(i + 1)\n",
    "            \n",
    "        globals()[pro_total_corpus_list_variable] = pro_total_corpus_list\n",
    "        globals()[post_total_corpus_list_variable] = post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_name = \"AIHUB_corpus/exploration/web_data_based_korean_corpus_data_pro/AIHUB_web_data_based_korean_corpus_data_\"\n",
    "\n",
    "topic_name_list = ['01_IT_과학', '02_건강', '03_경제', '04_교육', '05_국제', '06_라이프스타일', '07_문화',\n",
    "                  '08_사건사고', '09_사회일반', '10_산업', '11_스포츠', '12_여성복지', '13_여행레저',\n",
    "                  '14_연예', '15_정치', '16_지역', '17_취미']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_topic_pro_post_txt_file_name_list(pro_corpus_path, topic_name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성\n",
    "* Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    sentence = sentence.replace(\". . .\", \".\")\n",
    "    sentence = sentence.replace(\". .\", \".\")\n",
    "    sentence = sentence.replace(\"..\", \".\")\n",
    "    # 구두점 \". . .\", \". .\", \"..\"을 온점 \".\"으로 교체\n",
    "\n",
    "    ...\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬처리 라이브러리 ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(len(pro_total_corpus_list)))\n",
    "    print(\"\\n[Order]\")\n",
    "    num = 0\n",
    "    process_num = 10    \n",
    "\n",
    "    for pro, post in zip(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "        sentence_list = []\n",
    "\n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                results = ray.get(futures)\n",
    "\n",
    "                if i == nested_lines_num - 2:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "\n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "\n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로세서 개수만큼 코퍼스를 분리하여 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_list_train_01, post_total_corpus_list_train_01)\n",
    "\n",
    "...\n",
    "\n",
    "preprocessing_corpus_txt(pro_total_corpus_list_train_17, post_total_corpus_list_train_17)\n",
    "\n",
    "preprocessing_corpus_txt.(pro_total_corpus_list_valid_01, post_total_corpus_list_valid_01)\n",
    "\n",
    "...\n",
    "\n",
    "preprocessing_corpus_txt(pro_total_corpus_list_valid_17, post_total_corpus_list_valid_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_topic_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    corpus_list = glob(preprocessing_corpus_path)\n",
    "    corpus_list = sorted_list(corpus_list)\n",
    "    \n",
    "    merge_corpus_list = glob(merge_corpus_path + \"*.txt\")\n",
    "    \n",
    "    for i in range(len(topic_name_list)):\n",
    "        with open(merge_corpus_path + topic_name_list[i] + '.txt', 'w', encoding='utf-8') as f:\n",
    "            topic_corpus_list_with_none = [j if topic_name_list[i] in j else None for j in corpus_list]\n",
    "            topic_corpus_list = list(filter(None, topic_corpus_list))\n",
    "                    for corpus in topic_corpus_list:\n",
    "                        with open(corpus, encoding='utf-8') as text:\n",
    "                            for line in text:\n",
    "                                f.write(line)\n",
    "                    \n",
    "    with open(deduplicate_corpus_path, 'w', encoding='utf-8') as f1:\n",
    "        for corpus in merge_corpus_list:\n",
    "            with open(corpus, encoding='utf-8') as f2:\n",
    "                lines = f2.read().splitlines()\n",
    "                single_sentence_dict = dict.fromkeys(lines)\n",
    "                single_sentence_list = list(single_sentence_dict)\n",
    "                f1.write(\"\\n\".join(single_sentence_list))                         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/web_data_based_korean_corpus_data_post/AIHUB_web_data_based_korean_corpus_data_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_web_data_based_korean_corpus_data_'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_web_data_based_korean_corpus_data.txt'\n",
    "\n",
    "topic_name_list = ['01_IT_과학', '02_건강', '03_경제', '04_교육', '05_국제', '06_라이프스타일',\n",
    "                   '07_문화', '08_사건사고', '09_사회일반', '10_산업', '11_스포츠', '12_여성복지',\n",
    "                   '13_여행레저', '14_연예', '15_정치', '16_지역', '17_취미']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_topic_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                        deduplicate_corpus_path, topic_name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) 산업정보 연계 주요국 특허 영-한 데이터 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=563)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'labeled_data' - 'astrt_cont_kor'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_산업정보 연계 주요국 특허 영-한 데이터/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_산업정보 연계 주요국 특허 영-한 데이터/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/automatic_patent_classification_data_pro/AIHUB_automatic_patent_classification_data_train_\", \n",
    "                 \"exploration/automatic_patent_classification_data_pro/AIHUB_automatic_patent_classification_data_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_list = list(pd.DataFrame(one_json_sample['labeled_data'])['astrt_cont_kor'])\n",
    "\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_list = list(pd.DataFrame(one_json_sample['labeled_data'])['astrt_cont_kor'])\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'labeled_data' - 'astrt_cont_kor' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_pro/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_\" +\"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로세서 개수만큼 코퍼스를 분리하여 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_post/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_post/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) 논문자료 요약"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>논문요약_ ##  /  특허섹션_##\n",
    "<br>1) 'data' - 'summary_entire' -  'orginal_text'\n",
    "<br>2) 'data' - 'summary_section' -  'orginal_text'\n",
    "<br>\n",
    "<br>특허전체_##\n",
    "<br>'data' - 'summary_section' -  'orginal_text'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_논문자료 요약/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_논문자료 요약/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/summary_of_thesis_materials_pro/AIHUB_summary_of_thesis_materials_train_\", \n",
    "                 \"exploration/summary_of_thesis_materials_pro/AIHUB_summary_of_thesis_materials_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "\n",
    "    if '논문/논문요약' or '특허섹션' in source_file:\n",
    "            \n",
    "        try:\n",
    "            summary_entire_source_dict = dict(source_df['summary_entire'].explode())\n",
    "            summary_entire_source_json = pd.json_normalize(summary_entire_source_dict)\n",
    "            summary_entire_source_list = list(summary_entire_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "            summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "            summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "            summary_section_source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "            source_list = summary_entire_source_list + summary_section_source_list\n",
    "\n",
    "        except:\n",
    "            summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "            summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "            source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "                \n",
    "    else:\n",
    "        summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "        summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "        source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "\n",
    "    if '논문/논문요약' or '특허섹션' in source_file:\n",
    "            \n",
    "        try:\n",
    "            summary_entire_source_dict = dict(source_df['summary_entire'].explode())\n",
    "            summary_entire_source_json = pd.json_normalize(summary_entire_source_dict)\n",
    "            summary_entire_source_list = list(summary_entire_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "            summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "            summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "            summary_section_source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "            source_list = summary_entire_source_list + summary_section_source_list\n",
    "\n",
    "        except:\n",
    "            summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "            summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "            source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "                \n",
    "    else:\n",
    "        summary_section_source_dict = dict(source_df['summary_section'].explode())\n",
    "        summary_section_source_json = pd.json_normalize(summary_section_source_dict)\n",
    "        source_list = list(summary_section_source_json.filter(regex='orginal_text').values[0])\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>논문요약_ ##  /  특허섹션_##\n",
    "<br>1) 'data' - 'summary_entire' -  'orginal_text'\n",
    "<br>2) 'data' - 'summary_section' -  'orginal_text'\n",
    "<br>\n",
    "<br>특허전체_##\n",
    "<br>'data' - 'summary_section' -  'orginal_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/summary_of_thesis_materials_pro/AIHUB_summary_of_thesis_materials_\" + \"*.txt\"\n",
    "make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_post/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information_post/AIHUB_british_korean_data_on_patents_in_major_countries_linked_to_industrial_information.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) 요약문 및 레포트 생성 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=582)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명\n",
    "<br>len(list)가 Batch Size 이상일 때 까지 JSON 파일에서 리스트 병합"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'Meta(Refine)' – 'passage'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_json_txt_file_path_list(json_folder_list, topic_name_list, txt_path_list):\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder_list = ['AIHUB_요약문 및 레포트 생성 데이터/Training/원천데이터/TS1/',\n",
    "             'AIHUB_요약문 및 레포트 생성 데이터/Validation/원천데이터/VS1/']\n",
    "\n",
    "topic_name_list = ['01.news_r', '02.briefing', '03.his_cul', '04.paper', '05.minute',\n",
    "                   '06.edit', '07.public', '08.speech', '09.literature', '10.narration']\n",
    "\n",
    "txt_path_list = [\"exploration/summary_and_report_generation_data_pro/AIHUB_summary_and_report_generation_data_train_\", \n",
    "                 \"exploration/summary_and_report_generation_data_pro/AIHUB_summary_and_report_generation_data_valid_\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_topic_json_txt_file_path_list(json_folder_list, topic_name_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source = one_json_sample['Meta(Refine)']['passage']\n",
    "    \n",
    "    if (len(source_list) >= batch_size) or \\\n",
    "    (i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1)):\n",
    "        num += 1  \n",
    "        source_list = []\n",
    "    \n",
    "    source_list.append(source)\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(list)가 Batch Size 이상일 때까지 JSON 파일에서 리스트 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_total_txt_file_list):\n",
    "\n",
    "\n",
    "   ... \n",
    "\n",
    "    source = one_json_sample['Meta(Refine)']['passage']\n",
    "    \n",
    "    if len(source_list) >= batch_size:\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))   \n",
    "  \n",
    "        source_list = []\n",
    "    \n",
    "    elif i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1): \n",
    "        source_list.append(source)\n",
    "      \n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))  \n",
    "    \n",
    "    source_list.append(source)  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'Meta(Refine)' – 'passage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file_01, the_number_of_train_txt_file_list_01 = count_number_of_txt_file_with_batch_list(train_file_list_01, batch_size)\n",
    "\n",
    "...\n",
    "\n",
    "the_number_of_train_txt_file_10, the_number_of_train_txt_file_list_10 = count_number_of_txt_file_with_batch_list(train_file_list_10, batch_size)\n",
    "\n",
    "the_number_of_valid_txt_file_01, the_number_of_valid_txt_file_list_01 = count_number_of_txt_file_with_batch_listvalid_file_list_01, batch_size)\n",
    "\n",
    "...\n",
    "\n",
    "the_number_of_valid_txt_file_10, the_number_of_valid_txt_file_list_10 = count_number_of_txt_file_with_batch_list(valid_file_list_10, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list_01, train_text_file_path_list_01, batch_size, the_number_of_train_txt_file_list_01)\n",
    "...\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list_10, train_text_file_path_list_10, batch_size, the_number_of_train_txt_file_list_10)\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list_01, valid_text_file_path_list_01, batch_size, the_number_of_valid_txt_file_list_01)\n",
    "...\n",
    "\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list_10, valid_text_file_path_list_10, batch_size, the_number_of_valid_txt_file_list_10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_pro_post_txt_file_name_list(pro_corpus_path, topic_name_list):\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_name = \"AIHUB_corpus/exploration/summary_and_report_generation_data_pro/AIHUB_summary_and_report_generation_data_\"\n",
    "\n",
    "topic_name_list = ['01.news_r', '02.briefing', '03.his_cul', '04.paper', '05.minute',\n",
    "                   '06.edit', '07.public', '08.speech', '09.literature', '10.narration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_topic_pro_post_txt_file_name_list(pro_corpus_path, topic_name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성\n",
    "* Train, Valid 그리고 여러 가지 Topic으로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss3(source):\n",
    "\n",
    "    ...\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬처리 라이브러리 ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    process_num = 10  \n",
    "\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로세서 개수만큼 코퍼스를 분리하여 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_list_train_01, post_total_corpus_list_train_01)\n",
    "\n",
    "...\n",
    "\n",
    "preprocessing_corpus_txt(pro_total_corpus_list_train_10, post_total_corpus_list_train_10)\n",
    "\n",
    "preprocessing_corpus_txt.(pro_total_corpus_list_valid_01, post_total_corpus_list_valid_01)\n",
    "\n",
    "...\n",
    "\n",
    "preprocessing_corpus_txt(pro_total_corpus_list_valid_10, post_total_corpus_list_valid_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_topic_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...                       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/web_data_based_korean_corpus_data_post/AIHUB_web_data_based_korean_corpus_data_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_web_data_based_korean_corpus_data_'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_web_data_based_korean_corpus_data.txt'\n",
    "\n",
    "topic_name_list = ['01_IT_과학', '02_건강', '03_경제', '04_교육', '05_국제', '06_라이프스타일',\n",
    "                   '07_문화', '08_사건사고', '09_사회일반', '10_산업', '11_스포츠', '12_여성복지',\n",
    "                   '13_여행레저', '14_연예', '15_정치', '16_지역', '17_취미']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_topic_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                        deduplicate_corpus_path, topic_name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6) 문서요약 텍스트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=97)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'document' – 'text'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_문서요약 텍스트/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_문서요약 텍스트/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/document_summary_text_pro/AIHUB_document_summary_text_train_\", \n",
    "                 \"exploration/document_summary_text_pro/AIHUB_document_summary_text_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['documents'])\n",
    "    source_dict = dict(source_df['text'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)  \n",
    "    \n",
    "    source_list = []\n",
    "    for source_index in source_json:\n",
    "        for source_nested_list in source_json[source_index]:\n",
    "            for source_single_list in source_nested_list:\n",
    "                source_sentence = \"\"\n",
    "                for source_single in source_single_list:\n",
    "                    if type(source_single) == dict:\n",
    "                        for key, value in source_single.items():\n",
    "                            if key == \"sentence\":\n",
    "                                source_sentence += \" \" + value\n",
    "                if len(source_sentence) > 0:\n",
    "                    source_list.append(source_sentence)\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['documents'])\n",
    "    source_dict = dict(source_df['text'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)  \n",
    "    \n",
    "    source_list = []\n",
    "    for source_index in source_json:\n",
    "        for source_nested_list in source_json[source_index]:\n",
    "            for source_single_list in source_nested_list:\n",
    "                source_sentence = \"\"\n",
    "                for source_single in source_single_list:\n",
    "                    if type(source_single) == dict:\n",
    "                        for key, value in source_single.items():\n",
    "                            if key == \"sentence\":\n",
    "                                source_sentence += \" \" + value\n",
    "                if len(source_sentence) > 0:\n",
    "                    source_list.append(source_sentence)\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'document' – 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/document_summary_text_pro/AIHUB_document_summary_text_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/document_summary_text_post/AIHUB_document_summary_text_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_document_summary_text.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_document_summary_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7) 특허분야 자동분류 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=547)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'dataset' – 'abstract'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_특허 분야 자동분류 데이터/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_특허 분야 자동분류 데이터/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/automatic_patent_classification_data_pro/AIHUB_automatic_patent_classification_data_train_\", \n",
    "                 \"exploration/automatic_patent_classification_data_pro/AIHUB_automatic_patent_classification_data_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_list = list(pd.DataFrame(one_json_sample['dataset'])['abstract'].dropna())\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    source_list = list(pd.DataFrame(one_json_sample['dataset'])['abstract'].dropna())\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'dataset' – 'abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "  \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/automatic_patent_classification_data_pro/AIHUB_automatic_patent_classification_data_\" + \"*.txt\"\n",
    "pro_total_corpus_path_list, post_total_corpus_path_list = make_pro_post_txt_file_path_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/AIHUB_automatic_patent_classification_data_post/AIHUB_AIHUB_automatic_patent_classification_data_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_automatic_patent_classification_data.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_automatic_patent_classification_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8) 뉴스기사 기계독해 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=577)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_뉴스 기사 기계독해 데이터/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_뉴스 기사 기계독해 데이터/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/news_article_machine_reading_data_pro/AIHUB_news_article_machine_reading_data_train_\", \n",
    "                 \"exploration/news_article_machine_reading_data_pro/AIHUB_news_article_machine_reading_data_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/demo/news_article_machine_reading_data_pro/AIHUB_news_article_machine_reading_data_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/news_article_machine_reading_data_post/AIHUB_news_article_machine_reading_data_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_news_article_machine_reading_data.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_news_article_machine_reading_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9) 행정 문서 대상 기계독해 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=569)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_행정 문서 대상 기계독해 데이터/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_행정 문서 대상 기계독해 데이터/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/machine_reading_data_for_administrative_documents_pro/AIHUB_machine_reading_data_for_administrative_documents_train_\", \n",
    "                 \"exploration/machine_reading_data_for_administrative_documents_pro/AIHUB_machine_reading_data_for_administrative_documents_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ... \n",
    "\n",
    "        if 'tableqa' in source_file:\n",
    "\n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \"_tableqa.txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))   \n",
    "\n",
    "    else:\n",
    "\n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))   \n",
    "\n",
    "\n",
    "    ... \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/machine_reading_data_for_administrative_documents_pro/AIHUB_machine_reading_data_for_administrative_documents_train_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list\n",
    "\n",
    "@ray.remote\n",
    "def tableqa_preprocessing_text(source):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "\n",
    "    ...\n",
    "    \n",
    "                    if 'tableqa' in pro:\n",
    "                    futures = [tableqa_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [tableqa_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "                        \n",
    "                else:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/machine_reading_data_for_administrative_documents_post/AIHUB_machine_reading_data_for_administrative_documents_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_machine_reading_data_for_administrative_documents.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_machine_reading_data_for_administrative_documents.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10) 기계독해"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=89)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되지 않는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    file_path = sorted_list(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list)\n",
    "\n",
    "    file_list = json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(file_path) \n",
    "    print(\"The number of file:\", the_number_of_json_file)\n",
    "    \n",
    "    text_file_path_list = txt_file_path_list(train_file_path txt_path_list)\n",
    "\n",
    "    return file_list, text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_기계독해'+ '/**/*.json']\n",
    "txt_path_list =  [\"exploration/machine_reading_pro/AIHUB_machine_reading_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list, text_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되지 않는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/machine_reading_pro/AIHUB_machine_reading_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/machine_reading_post/AIHUB_machine_reading_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_machine_reading.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_machine_reading.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11) 도서자료 요약"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=93)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>1) 'passage'\n",
    "<br>2) 'summary' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_도서자료 요약/Training/'+ '/**/*.json', \n",
    "                'AIHUB_도서자료 요약/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/summary_of_book_materials_pro/AIHUB_summary_of_book_materials_train_\", \n",
    "                 \"exploration/summary_of_book_materials_pro/AIHUB_summary_of_book_materials_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    passage = one_json_sample[\"passage\"]  \n",
    "    summary = one_json_sample[\"summary\"]\n",
    "\n",
    "    if len(source_list) >= batch_size or \\\n",
    "        (i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1)):\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")    \n",
    "\n",
    "        source_list = []\n",
    "\n",
    "    source_list.append(passage)   \n",
    "    source_list.append(summary)         \n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(list)가 Batch Size 이상일 때까지 JSON 파일에서 리스트 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    passage = one_json_sample[\"passage\"]  \n",
    "    summary = one_json_sample[\"summary\"]\n",
    "\n",
    "    if len(source_list) >= batch_size:\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))   \n",
    "  \n",
    "        source_list = []\n",
    "    \n",
    "    elif i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1): \n",
    "        source_list.append(passage)   \n",
    "        source_list.append(summary)   \n",
    "      \n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))  \n",
    "    \n",
    "    source_list.append(passage)   \n",
    "    source_list.append(summary)              \n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>1) 'passage'\n",
    "<br>2) 'summary' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def professional_corpus_post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/summary_of_book_materials_pro/AIHUB_summary_of_book_materials_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss3(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/summary_of_book_materials_post/AIHUB_summary_of_book_materials_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_summary_of_book_materials.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_summary_of_book_materials.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12) 대규모 구매도서 기반 한국어 말뭉치 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'paragraphs' – 'sentences'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/000/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/korean_corpus_data_based_on_large_scale_purchase_books_pro/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['paragraphs'])\n",
    "    source_dict = dict(source_df['sentences'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)  \n",
    "    \n",
    "    source_list = []\n",
    "    for source_index in source_json:\n",
    "        for source_nested_list in source_json[source_index]:\n",
    "\n",
    "            try:\n",
    "                for source_single_list in source_nested_list:\n",
    "                    try:\n",
    "                        for key, value in source_single_list.items():\n",
    "                            if key == 'text':\n",
    "                                source_list.append(value)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass  \n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['paragraphs'])\n",
    "    source_dict = dict(source_df['sentences'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)  \n",
    "    \n",
    "    source_list = []\n",
    "    for source_index in source_json:\n",
    "        for source_nested_list in source_json[source_index]:\n",
    "\n",
    "            try:\n",
    "                for source_single_list in source_nested_list:\n",
    "                    try:\n",
    "                        for key, value in source_single_list.items():\n",
    "                            if key == 'text':\n",
    "                                source_list.append(value)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass  \n",
    "\n",
    "    ... \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'paragraphs' – 'sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def professional_corpus_post_txt_file_path_list(corpus_list):\n",
    "  \n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/korean_corpus_data_based_on_large_scale_purchase_books_pro/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss3(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/korean_corpus_data_based_on_large_scale_purchase_books_post/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13) 도서자료 기계독해"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=92)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되지 않는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    file_path = sorted_list(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list)\n",
    "\n",
    "    ...\n",
    "\n",
    "    return file_list, text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_기계독해'+ '/**/*.json']\n",
    "txt_path_list =  [\"exploration/machine_reading_pro/AIHUB_machine_reading_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list, text_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되지 않는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    source_filter_02 = source_json.filter(regex='[0123456789]').values[0]\n",
    "    source_df_02 = pd.DataFrame(source_filter_02)\n",
    "    \n",
    "    for source_df_02_value in source_df_02.values:\n",
    "        for value in source_df_02_value[0]:\n",
    "        try:\n",
    "            source_list.append(value['context'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  \n",
    "    ...\n",
    "\n",
    "    source_df = pd.DataFrame(one_json_sample['data'])\n",
    "    source_dict = dict(source_df['paragraphs'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)\n",
    "    source_list = list(source_json.filter(regex='context').values[0])\n",
    "\n",
    "    source_filter_02 = source_json.filter(regex='[0123456789]').values[0]\n",
    "    source_df_02 = pd.DataFrame(source_filter_02)\n",
    "    \n",
    "    for source_df_02_value in source_df_02.values:\n",
    "        for value in source_df_02_value[0]:\n",
    "        try:\n",
    "            source_list.append(value['context'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>'data' – 'paragraph' – 'context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/reading_books_by_machine_pro/AIHUB_reading_books_by_machine_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/reading_books_by_machine_post/AIHUB_reading_books_by_machine_\" + \"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_reading_books_by_machine.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_reading_books_by_machine.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.14) 법률 규정 (판결서 약관 등) 텍스트 분석 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=580)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br>판결문\n",
    "<br>1) 'disposal' - 'disposalcontent'\n",
    "<br>2) 'mentionedItems' - 'rqestObjet'\n",
    "<br>3) 'assrs' - 'acusrAssrs'\n",
    "<br>4) 'assrs' - 'dedatAssrs'\n",
    "<br>5) 'facts' - 'bsisFacts'\n",
    "<br>6) 'dcss' - 'courtDcss'\n",
    "<br>7) 'close' - 'cnclsns'\n",
    "\n",
    "약관 – 01.유리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) 'comProvision'\n",
    "\n",
    "약관 – 02.불리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) ''illdcssBasiss'\n",
    "<br>3) 'relateLaword'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "    ...\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "   ...\n",
    "\n",
    "    return train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_법률 규정 (판결서 약관 등) 텍스트 분석 데이터/Training/' + '/**/*.json',\n",
    "             'AIHUB_법률 규정 (판결서 약관 등) 텍스트 분석 데이터/Validation/' + '/**/*.json']\n",
    "txt_path_list = [\"exploration/legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_pro/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_train_\", \n",
    "                 \"exploration/legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_pro/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]   \n",
    "            \n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file) \n",
    "\n",
    "        if '판결문' in source_file:\n",
    "            source_df = pd.DataFrame(one_json_sample)\n",
    "            sources = source_df.loc['disposalcontent']['disposal'] + \\\n",
    "                source_df.loc['rqestObjet']['mentionedItems'] + \\\n",
    "                source_df.loc['acusrAssrs']['assrs'] + \\\n",
    "                source_df.loc['dedatAssrs']['assrs'] + \\\n",
    "                source_df.loc['bsisFacts']['facts'] + \\\n",
    "                source_df.loc['courtDcss']['dcss'] + \\\n",
    "                source_df.loc['cnclsns']['close']\n",
    "\n",
    "        elif '유리' in source_file:\n",
    "            source_df = pd.DataFrame.from_dict(one_json_sample, orient='index')\n",
    "            source_df = source_df.transpose()\n",
    "            sources = [source_df['clauseArticle'][0], source_df['comProvision'][0]]\n",
    "\n",
    "        elif '불리' in source_file:\n",
    "            source_df = pd.DataFrame.from_dict(one_json_sample, orient='index')\n",
    "            source_df = source_df.transpose()\n",
    "            sources = [source_df['clauseArticle'][0][0], source_df['illdcssBasiss'][0][0],\n",
    "                            source_df['relateLaword'][0][0]]\n",
    "\n",
    "        if len(source_list) >= batch_size or \\\n",
    "        (i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1)):\n",
    "        num += 1  \n",
    "        source_list = []\n",
    "            \n",
    "        for source in sources:\n",
    "            source_list.append(source)                        \n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(list)가 Batch Size 이상일 때까지 JSON 파일에서 리스트 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    if '판결문' in source_file:\n",
    "    source_df = pd.DataFrame(one_json_sample)\n",
    "    sources = source_df.loc['disposalcontent']['disposal'] + \\\n",
    "    source_df.loc['rqestObjet']['mentionedItems'] + \\\n",
    "    source_df.loc['acusrAssrs']['assrs'] + \\\n",
    "    source_df.loc['dedatAssrs']['assrs'] + \\\n",
    "    source_df.loc['bsisFacts']['facts'] + \\\n",
    "    source_df.loc['courtDcss']['dcss'] + \\\n",
    "    source_df.loc['cnclsns']['close']\n",
    "    \n",
    "    elif '유리' in source_file: \n",
    "    source_df = pd.DataFrame.from_dict(one_json_sample, orient='index')\n",
    "    source_df = source_df.transpose()\n",
    "    sources = [source_df['clauseArticle'][0], source_df['comProvision'][0]]\n",
    "\n",
    "    elif '불리' in source_file: \n",
    "    source_df = pd.DataFrame.from_dict(one_json_sample, orient='index')\n",
    "    source_df = source_df.transpose()\n",
    "    sources = [source_df['clauseArticle'][0][0], source_df['illdcssBasiss'][0][0],\n",
    "                    source_df['relateLaword'][0][0]]\n",
    "\n",
    "    if len(source_list) >= batch_size:\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list))   \n",
    "\n",
    "        source_list = []\n",
    "        \n",
    "    elif i == (len(source_file_nested_list) -1) and j == (len(source_file_list) -1): \n",
    "        for source in sources\n",
    "        source_list.append(source)\n",
    "    \n",
    "        num += 1\n",
    "        print(str(num), end=\" \") \n",
    "        \n",
    "        with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list)) \n",
    "            \n",
    "    for source in sources:\n",
    "    source_list.append(source)        \n",
    "\n",
    "\n",
    "    ... \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br>판결문\n",
    "<br>1) 'disposal' - 'disposalcontent'\n",
    "<br>2) 'mentionedItems' - 'rqestObjet'\n",
    "<br>3) 'assrs' - 'acusrAssrs'\n",
    "<br>4) 'assrs' - 'dedatAssrs'\n",
    "<br>5) 'facts' - 'bsisFacts'\n",
    "<br>6) 'dcss' - 'courtDcss'\n",
    "<br>7) 'close' - 'cnclsns'\n",
    "\n",
    "약관 – 01.유리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) 'comProvision'\n",
    "\n",
    "약관 – 02.불리\n",
    "<br>1) 'clauseArticle'\n",
    "<br>2) ''illdcssBasiss'\n",
    "<br>3) 'relateLaword'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_pro/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "        \n",
    "    sources = re.split(r\"[0-9]+[.]|①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩|⑪|⑫|⑬|⑭|⑮\", source)\n",
    "    # 숫자.(1. 2. ...), 원숫자(①, ②, ③, ...)를 기준으로 문장 분리\n",
    "\n",
    "    ...\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이후 TXT 파일 병합 및 중복 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_post/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.15) 일반상식"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=106)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Valid로 구분되지 않는 Json 파일명\n",
    "<br>전체 리스트를 Batch Size에 따라 분할 (len(list) >= Batch Size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>ko_wiki_v1_squad.json\n",
    "<br>'data' – 'paragraph' – 'context'\n",
    "\n",
    "<br>edited_####.json\n",
    "<br>'sentence' - 'text'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "   \n",
    "   ...\n",
    "   \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    file_path = sorted_list(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    file_path = sorted_list(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  ...\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list)\n",
    "\n",
    "    ...\n",
    "\n",
    "    return file_list, text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_일반상식/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/general_common_sense_pro/AIHUB_general_common_sense_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list, text_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 JSON 파일과 추출 TXT 파일 경로 리스트 생성\n",
    "- Train, Valid로 구분되지 않는 Json 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    if 'ko_wiki_v1_squad' in source_file:\n",
    "        source_df = pd.DataFrame(one_json_sample['data'])\n",
    "        source_dict = dict(source_df['paragraphs'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='context').values[0])\n",
    "        \n",
    "    else:\n",
    "        source_list = (pd.DataFrame(one_json_sample['sentence'])['text'])\n",
    "\n",
    "    ...\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 개수 Count에 따라 Batch Size 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list, text_file_path_list, batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    if 'ko_wiki_v1_squad' in source_file:\n",
    "        source_df = pd.DataFrame(one_json_sample['data'])\n",
    "        source_dict = dict(source_df['paragraphs'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='context').values[0])\n",
    "        \n",
    "    else:\n",
    "        source_list = (pd.DataFrame(one_json_sample['sentence'])['text'])\n",
    "\n",
    "    ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON에서 추출한 텍스트를 Batch Size로 TXT 파일에 나누어 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key\n",
    "<br><br>ko_wiki_v1_squad.json\n",
    "<br>'data' – 'paragraph' – 'context'\n",
    "\n",
    "<br>edited_####.json\n",
    "<br>'sentence' - 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_path_list(corpus_list):\n",
    "   \n",
    "  ...\n",
    "\n",
    "  return post_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    return pro_total_corpus_list, post_total_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/general_common_sense_pro/AIHUB_general_common_sense_\" + \"*.txt\"\n",
    "pro_total_corpus_list, post_total_corpus_list = make_pro_post_txt_file_name_list(pro_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 이전과 이후 TXT 파일 경로 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text_kss1(source):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_list, post_total_corpus_list):\n",
    "\n",
    "    ...\n",
    "\n",
    "    process_num = 10 \n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT 파일 1개씩 전처리 작업 수행\n",
    "<br>- 작업 진행 상황 확인: 현재까지 전처리한 TXT 파일 개수 Print\n",
    "<br>- 병렬 처리: 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    ...\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/general_common_sense/AIHUB_general_common_sense_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_general_common_sense.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_general_common_sense.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
