{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHub Json Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kss==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS Argument Error: Restart Jupyter Kernel Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-mecab-ko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 3.7.3 matches python-mecab-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import kss\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from mecab import MeCab\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AIHUB'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "    \n",
    "    path_list = sorted(path_list, reverse=False)\n",
    "    path_list = sorted(path_list, key=len)\n",
    "    \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_source_file_list(l, n): \n",
    "    \n",
    "  for i in range(0, len(l), n): \n",
    "    yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    json_file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    json_file_path = sorted_list(json_file_path)\n",
    "    \n",
    "    return json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "  train_json_file_path, valid_json_file_path = [glob(i, recursive = True) if 'rain' in i\n",
    "                                      else glob(i, recursive = True)\n",
    "                                      for i in path_list]\n",
    "\n",
    "  train_json_file_path = sorted_list(train_json_file_path)\n",
    "  valid_json_file_path = sorted_list(valid_json_file_path)\n",
    "    \n",
    "  return train_json_file_path, valid_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(json_file_path, folder_corpus_type_path):\n",
    "  \n",
    "  text_json_file_path = [folder_corpus_type_path + str(i) + \".txt\"\n",
    "                              for i in range(len(json_file_path))]\n",
    "    \n",
    "  return text_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    train_json_file_path, valid_json_file_path = train_valid_json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_train_json_file  = len(train_json_file_path)\n",
    "    the_number_of_valid_json_file  = len(valid_json_file_path)\n",
    "    the_number_of_json_file = the_number_of_train_json_file + the_number_of_valid_json_file\n",
    "    print(\"The number of train json file:\", the_number_of_train_json_file)\n",
    "    print(\"The number of valid json file:\", the_number_of_valid_json_file)\n",
    "    print(\"The number of json file:\", the_number_of_json_file)\n",
    "\n",
    "    train_txt_file_path = txt_file_path_list(train_json_file_path, txt_path_list[0])\n",
    "    valid_txt_file_path = txt_file_path_list(valid_json_file_path, txt_path_list[1])\n",
    "\n",
    "    return train_json_file_path, valid_json_file_path, train_txt_file_path, valid_txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    json_file_path = json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(json_file_path) \n",
    "    print(\"The number of json file:\", the_number_of_json_file)\n",
    "    \n",
    "    txt_file_path = txt_file_path_list(json_file_path, txt_path_list[0])\n",
    "\n",
    "    return json_file_path, txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "    \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "        if source[0] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[-1] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[0] == '\"' and source[-1] == '\"':\n",
    "            del(source[0])\n",
    "            del(source[-1])\n",
    "        # 문장의 시작과 끝이 따옴표(\"\")이면 따옴표 제거\n",
    "        \n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence) \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "\n",
    "                for sentence2 in kss.split_sentences(sentence, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "                    for sentence3 in kss.split_sentences(sentence2, use_heuristic=False,\n",
    "                                                         num_workers=32):\n",
    "                        preprocessing_sentence_list.append(sentence3)\n",
    "\n",
    "            # 마지막에 KSS(Korean Sentence Segmentation)로 문장 분리 2번 실행\n",
    "\n",
    "  \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIHUB 대규모 구매도서 기반 한국어 말뭉치 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON File to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/000/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/korean_corpus_data_based_on_large_scale_purchase_books_pro/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of file: 51\n"
     ]
    }
   ],
   "source": [
    "json_file_list, txt_file_path_list = \\\n",
    "    make_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_index_df = pd.DataFrame(json_file_list, columns=['source_file_name'])\n",
    "source_file_index_df.to_excel(\"source_file_index/korean_corpus_data_based_on_large_scale_purchase_books_source_file_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_source_list(json_sample):\n",
    "\n",
    "    source_df = pd.DataFrame(json_sample['paragraphs'])\n",
    "    source_dict = dict(source_df['sentences'].explode())\n",
    "    source_json = pd.json_normalize(source_dict)  \n",
    "    \n",
    "    source_list = []\n",
    "    for source_index in source_json:\n",
    "        for source_nested_list in source_json[source_index]:\n",
    "\n",
    "            try:\n",
    "                for source_single_list in source_nested_list:\n",
    "                    try:\n",
    "                        for key, value in source_single_list.items():\n",
    "                            if key == 'text':\n",
    "                                source_list.append(value)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass \n",
    "    \n",
    "    return source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "\n",
    "    source_file_by_batch_df = pd.DataFrame({'File':[0], 'Length of Source List':[0],\n",
    "                                        'The Number of TXT File':[0], \n",
    "                                        'Description':[0]})\n",
    "                                            \n",
    "    the_number_of_total_txt_file = 0\n",
    "    the_number_of_txt_file_list = []\n",
    "    \n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]   \n",
    "\n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file)\n",
    "\n",
    "        try:\n",
    "            source_list = make_source_list(one_json_sample)\n",
    "        \n",
    "            the_number_of_txt_file = ((len(source_list) // batch_size) + 1)\n",
    "\n",
    "            if len(source_list) >= batch_size:\n",
    "                source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                                len(source_list), the_number_of_txt_file, \"\"]\n",
    "                the_number_of_txt_file_list.append(the_number_of_txt_file)\n",
    "                the_number_of_total_txt_file  += the_number_of_txt_file\n",
    "\n",
    "            elif len(source_list) < batch_size:\n",
    "                source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                                len(source_list), the_number_of_txt_file,\n",
    "                                                \"not subject of batch. small source list.\"]\n",
    "                the_number_of_txt_file_list.append(1)\n",
    "                the_number_of_total_txt_file  += 1\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"The number of txt file:\", the_number_of_total_txt_file)\n",
    "\n",
    "    if 'rain' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/korean_corpus_data_based_on_large_scale_purchase_books_train.xlsx\", index=False)\n",
    "    elif 'alid' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/korean_corpus_data_based_on_large_scale_purchase_books_valid.xlsx\", index=False)\n",
    "    else:\n",
    "         source_file_by_batch_df.to_excel(\"source_file_by_batch/korean_corpus_data_based_on_large_scale_purchase_books.xlsx\", index=False)\n",
    "\n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_txt_file_list):\n",
    "  \n",
    "  \n",
    "  progress_length = sum(the_number_of_txt_file_list)\n",
    "  print(\"[Size]\")\n",
    "  print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "  print(\"\\n[Order]\")\n",
    "  pbar = tqdm(range(progress_length))\n",
    "  num = 0\n",
    "\n",
    "  for i in range(len(source_file_list)):\n",
    "\n",
    "    source_file = source_file_list[i]\n",
    "    \n",
    "    with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "      one_json_sample = json.load(one_json_file)\n",
    "      \n",
    "    try:\n",
    "      source_list = make_source_list(one_json_sample)\n",
    "    \n",
    "      n = batch_size\n",
    "      source_batch_list = list(divide_source_file_list(source_list, n))\n",
    "        \n",
    "      for source_list in source_batch_list:\n",
    "          with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "            fp.write(\"\\n\".join(source_list)) \n",
    "          num += 1  \n",
    "          pbar.n += 1\n",
    "          pbar.refresh()\n",
    "          time.sleep(0.01)\n",
    "          \n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  pbar.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 100\n",
      "The number of txt file: 8900\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "the_number_of_txt_file, the_number_of_txt_file_list = count_number_of_txt_file_with_batch_list(json_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>File</th>\n",
       "      <th>Length of Source List</th>\n",
       "      <th>The Number of txt File</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17494</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18269</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>20049</td>\n",
       "      <td>201</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18494</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17708</td>\n",
       "      <td>178</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18290</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17544</td>\n",
       "      <td>176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17932</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17902</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17958</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18693</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18129</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18316</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18028</td>\n",
       "      <td>181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18010</td>\n",
       "      <td>181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17931</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18536</td>\n",
       "      <td>186</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18241</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18349</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18131</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18201</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17534</td>\n",
       "      <td>176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17758</td>\n",
       "      <td>178</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17824</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18161</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18127</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17898</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17752</td>\n",
       "      <td>178</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17580</td>\n",
       "      <td>176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18031</td>\n",
       "      <td>181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18386</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17497</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17497</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17844</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18274</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17994</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17831</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18026</td>\n",
       "      <td>181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17563</td>\n",
       "      <td>176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17917</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18336</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18161</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18407</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>18575</td>\n",
       "      <td>186</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17494</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17719</td>\n",
       "      <td>178</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17516</td>\n",
       "      <td>176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17996</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>17683</td>\n",
       "      <td>177</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...</td>\n",
       "      <td>3925</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               File  \\\n",
       "0            0                                                  0   \n",
       "1            1  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "2            2  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "3            3  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "4            4  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "5            5  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "6            6  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "7            7  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "8            8  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "9            9  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "10          10  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "11          11  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "12          12  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "13          13  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "14          14  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "15          15  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "16          16  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "17          17  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "18          18  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "19          19  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "20          20  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "21          21  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "22          22  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "23          23  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "24          24  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "25          25  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "26          26  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "27          27  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "28          28  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "29          29  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "30          30  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "31          31  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "32          32  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "33          33  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "34          34  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "35          35  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "36          36  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "37          37  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "38          38  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "39          39  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "40          40  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "41          41  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "42          42  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "43          43  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "44          44  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "45          45  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "46          46  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "47          47  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "48          48  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "49          49  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "50          50  AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/sample/라벨링데이터/00...   \n",
       "\n",
       "    Length of Source List  The Number of txt File  Description  \n",
       "0                       0                       0          0.0  \n",
       "1                   17494                     175          NaN  \n",
       "2                   18269                     183          NaN  \n",
       "3                   20049                     201          NaN  \n",
       "4                   18494                     185          NaN  \n",
       "5                   17708                     178          NaN  \n",
       "6                   18290                     183          NaN  \n",
       "7                   17544                     176          NaN  \n",
       "8                   17932                     180          NaN  \n",
       "9                   17902                     180          NaN  \n",
       "10                  17958                     180          NaN  \n",
       "11                  18693                     187          NaN  \n",
       "12                  18129                     182          NaN  \n",
       "13                  18316                     184          NaN  \n",
       "14                  18028                     181          NaN  \n",
       "15                  18010                     181          NaN  \n",
       "16                  17931                     180          NaN  \n",
       "17                  18536                     186          NaN  \n",
       "18                  18241                     183          NaN  \n",
       "19                  18349                     184          NaN  \n",
       "20                  18131                     182          NaN  \n",
       "21                  18201                     183          NaN  \n",
       "22                  17534                     176          NaN  \n",
       "23                  17758                     178          NaN  \n",
       "24                  17824                     179          NaN  \n",
       "25                  18161                     182          NaN  \n",
       "26                  18127                     182          NaN  \n",
       "27                  17898                     179          NaN  \n",
       "28                  17752                     178          NaN  \n",
       "29                  17580                     176          NaN  \n",
       "30                  18031                     181          NaN  \n",
       "31                  18386                     184          NaN  \n",
       "32                  17497                     175          NaN  \n",
       "33                  17497                     175          NaN  \n",
       "34                  17844                     179          NaN  \n",
       "35                  18274                     183          NaN  \n",
       "36                  17994                     180          NaN  \n",
       "37                  17831                     179          NaN  \n",
       "38                  18026                     181          NaN  \n",
       "39                  17563                     176          NaN  \n",
       "40                  17917                     180          NaN  \n",
       "41                  18336                     184          NaN  \n",
       "42                  18161                     182          NaN  \n",
       "43                  18407                     185          NaN  \n",
       "44                  18575                     186          NaN  \n",
       "45                  17494                     175          NaN  \n",
       "46                  17719                     178          NaN  \n",
       "47                  17516                     176          NaN  \n",
       "48                  17996                     180          NaN  \n",
       "49                  17683                     177          NaN  \n",
       "50                   3925                      40          NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_by_batch_df = pd.read_excel('source_file_by_batch/korean_corpus_data_based_on_large_scale_purchase_books.xlsx', engine='openpyxl')  \n",
    "source_file_by_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of preprocessing corpus: 8900\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8900/8900 [06:52<00:00, 21.59it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "write_jsontext_to_txt_file_with_batch_list(json_file_list, txt_file_path_list,\n",
    "                batch_size, the_number_of_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_txt_file_name_list(corpus_path_list):\n",
    "   \n",
    "  post_corpus_path_list = [corpus_file.replace(\"pro\", \"post\")\n",
    "                      for corpus_file in corpus_path_list]\n",
    "\n",
    "  return post_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    pro_total_corpus_path_list = glob(pro_corpus_path)\n",
    "    pro_total_corpus_path_list = sorted_list(pro_total_corpus_path_list)\n",
    "    post_total_corpus_path_list = post_txt_file_name_list(pro_total_corpus_path_list)\n",
    "\n",
    "    return pro_total_corpus_path_list, post_total_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/korean_corpus_data_based_on_large_scale_purchase_books_pro/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\" + \"*.txt\"\n",
    "pro_total_corpus_path_list, post_total_corpus_path_list = make_pro_post_txt_file_path_list(pro_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pro_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines() \n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:\n",
    "           line_list.append(line)\n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:  \n",
    "            sentences = formal_preprocessing_text(line)\n",
    "            for sentence in sentences:\n",
    "                line_list.append(sentence) \n",
    "            \n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "    \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "        if source[0] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[-1] == '\"':\n",
    "            del(source[0])\n",
    "        elif source[0] == '\"' and source[-1] == '\"':\n",
    "            del(source[0])\n",
    "            del(source[-1])\n",
    "        # 문장의 시작과 끝이 따옴표(\"\")이면 따옴표 제거\n",
    "        \n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence)    \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "\n",
    "                for sentence2 in kss.split_sentences(sentence, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "                    for sentence3 in kss.split_sentences(sentence2, use_heuristic=False,\n",
    "                                                         num_workers=32):\n",
    "                        preprocessing_sentence_list.append(sentence3)\n",
    "\n",
    "            # 마지막에 KSS(Korean Sentence Segmentation)로 문장 분리 2번 실행\n",
    "\n",
    "  \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "    \n",
    "    progress_length = len(pro_total_corpus_path_list)\n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "    print(\"\\n[Order]\")\n",
    "    pbar = tqdm(range(progress_length))\n",
    "    process_num = 10    \n",
    "\n",
    "    for pro, post in zip(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "\n",
    "        sentence_list = []\n",
    "\n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                results = ray.get(futures)\n",
    "\n",
    "                if i == nested_lines_num - 2:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "\n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "\n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))\n",
    "\n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    pbar.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    corpus_list = glob(preprocessing_corpus_path)\n",
    "    corpus_list = sorted_list(corpus_list)\n",
    "    \n",
    "    with open(merge_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for corpus in corpus_list:\n",
    "            with open(corpus, encoding='utf-8') as text:\n",
    "                for line in text:\n",
    "                    f.write(line)\n",
    "                    \n",
    "    with open(deduplicate_corpus_path, 'w', encoding='utf-8') as f1:\n",
    "        with open(merge_corpus_path, encoding='utf-8') as f2:\n",
    "            lines = f2.read().splitlines()\n",
    "            single_sentence_dict = dict.fromkeys(lines)\n",
    "            single_sentence_list = list(single_sentence_dict)\n",
    "            f1.write(\"\\n\".join(single_sentence_list))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/korean_corpus_data_based_on_large_scale_purchase_books_post/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_korean_corpus_data_based_on_large_scale_purchase_books.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, \n",
    "                                  deduplicate_corpus_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus_05",
   "language": "python",
   "name": "corpus_05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
