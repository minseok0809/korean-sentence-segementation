{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fxvrJ9Mh_tpU"
      },
      "source": [
        "## AIHub Json Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsJCZ9Cz_7Hk"
      },
      "source": [
        "### Development Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-PyJ1RONvVc"
      },
      "outputs": [],
      "source": [
        "%pip install kss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxZh_TEjNuRn"
      },
      "outputs": [],
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y8EK97Ab_6y0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import kss\n",
        "import json\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import konlpy\n",
        "from konlpy.tag import Mecab\n",
        "from lxml import etree\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JvJZGKtBmX8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zDzxsYB5_21r"
      },
      "source": [
        "### AIHUB 일반상식"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=106)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ybYfTDvD6Rd"
      },
      "outputs": [],
      "source": [
        "def json_file_name_list(path_list):\n",
        "    for i in path_list:\n",
        "        file_name = glob(i, recursive = True)\n",
        "    return file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAUv-1O2DP_8"
      },
      "outputs": [],
      "source": [
        "path_list = ['/content/drive/MyDrive/AIHUB/AIHUB_일반상식/'+ '/**/*.json']\n",
        "file_name = json_file_name_list(path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AoV5edaEW1M"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(file_name_list, corpus_file_name):\n",
        "\n",
        "  sentence_list = []\n",
        "\n",
        "  for i in range(len(file_name_list)):\n",
        "      with open(file_name_list[i], 'r', encoding='utf-8') as one_json_file:\n",
        "          one_json_sample = json.load(one_json_file)\n",
        "\n",
        "          with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:\n",
        "            if 'ko_wiki_v1_squad' in file_name_list[i]:\n",
        "                for j in one_json_sample['data']:\n",
        "                  for sentence in kss.split_sentences(j['paragraphs'][0]['context']):\n",
        "                    if sentence[-1] == \".\":\n",
        "                      if sentence != \".\":\n",
        "                          sentence_list.append(sentence) \n",
        "            else:\n",
        "              for j in one_json_sample['sentence']:\n",
        "                  for sentence in kss.split_sentences(j['text']):\n",
        "                    if sentence[-1] == \".\":\n",
        "                      if sentence != \".\":\n",
        "                          sentence_list.append(sentence) \n",
        "\n",
        "  with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:       \n",
        "      fp.write(\"\\n\".join(sentence_list)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDC0eYwMCnkU"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_general_common_sense.txt\"\n",
        "make_corpus_txt(file_name, corpus_file_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nLmPhykSYBfK"
      },
      "source": [
        "### AIHUB 법률 규정 (판결서 약관 등) 텍스트 분석 데이터"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=116&topMenu=100&aihubDataSe=ty&dataSetSn=580)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TLMijRfX3vnZ"
      },
      "outputs": [],
      "source": [
        "def xml_file_name_list(path_list):\n",
        "    for i in path_list:\n",
        "        if 'rain' in i:\n",
        "            train_file_name = glob(i, recursive = True)\n",
        "        elif 'alid' in i:  \n",
        "            valid_file_name = glob(i, recursive = True)\n",
        "    return train_file_name, valid_file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0CjOamAR5_7o"
      },
      "outputs": [],
      "source": [
        "path_list = ['/content/drive/MyDrive/AIHUB/AIHUB_법률 규정 (판결서 약관 등) 텍스트 분석 데이터/Training/' + '/**/*.xml',\n",
        "             '/content/drive/MyDrive/AIHUB/AIHUB_법률 규정 (판결서 약관 등) 텍스트 분석 데이터/Validation/' + '/**/*.xml']\n",
        "\n",
        "train_file_name, valid_file_name = xml_file_name_list(path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fGSWP5Rn_VuB"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(file_name_list, corpus_file_name):\n",
        "\n",
        "  sentence_list = []\n",
        "  parser = etree.XMLParser(remove_blank_text=True,recover=True) \n",
        "\n",
        "  for i in range(len(file_name_list)):\n",
        "    try:\n",
        "      root = etree.XML(open(file_name_list[i],'r').read().encode('utf-8'),parser) \n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    if '약관' in file_name_list[i]:\n",
        "      try:\n",
        "        root_text = root[0][2].text\n",
        "        for sentence in kss.split_sentences(re.sub(r\"\\n|제+[0-9]+조+ (\\([^)]*\\))|00000 약관|[0-9].  |\", \"\", str(root_text))):\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "            sentence_list.append(sentence)  \n",
        "\n",
        "      except IndexError:\n",
        "        pass\n",
        "            \n",
        "      except TypeError:\n",
        "        pass\n",
        "      \n",
        "\n",
        "    elif '01.민사' in file_name_list[i]:\n",
        "      num = 0\n",
        "      try:\n",
        "        root_text = root[0][2].text\n",
        "        for sentence in kss.split_sentences(' '.join(re.sub(r\"\\n\", \"\", str(root_text)).split())):\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "            if '2. 원고의 청구에 대한 판단' in sentence:\n",
        "              num += 1\n",
        "              if num == 1 and '2. 원고의 청구에 대한 판단' not in sentence:\n",
        "                sentence_list.append(sentence)  \n",
        "      except IndexError:\n",
        "        pass\n",
        "            \n",
        "      except TypeError:\n",
        "        pass\n",
        "\n",
        "    elif '02.형사' in file_name_list[i]:\n",
        "      num = 0\n",
        "      try:\n",
        "        root_text = root[0][2].text\n",
        "        for sentence in kss.split_sentences(' '.join(re.sub(r\"\\n\", \"\", str(root_text)).split())):\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "            if '판례 검색' in sentence:\n",
        "              num += 1\n",
        "              if num == 1 and '판례 검색' not in sentence:\n",
        "                sentence_list.append(sentence)  \n",
        "\n",
        "      except IndexError:\n",
        "        pass\n",
        "            \n",
        "      except TypeError:\n",
        "        pass\n",
        "\n",
        "\n",
        "    elif '03.행정' in file_name_list[i]:\n",
        "      num = 0\n",
        "      try:\n",
        "        root_text = root[0][2].text\n",
        "        for sentence in kss.split_sentences(' '.join(re.sub(r\"\\n\", \"\", root[0][2].text).split())):\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                if '관계 법령' in sentence:\n",
        "                  num += 1\n",
        "                  if num == 1 and '관계 법령' not in sentence:\n",
        "                    sentence_list.append(sentence)  \n",
        "\n",
        "      except IndexError:\n",
        "        pass\n",
        "            \n",
        "      except TypeError:\n",
        "        pass\n",
        "\n",
        "  only_sentence_list = list(OrderedDict.fromkeys(sentence_list))\n",
        "  with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:       \n",
        "      fp.write(\"\\n\".join(sentence_list)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L4pHTjFnZsVK"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_legal_regulations_(such_as_terms_and_conditions_of_judgment)_text_analysis_data.txt\"\n",
        "make_corpus_txt(train_file_name, corpus_file_name)\n",
        "make_corpus_txt(valid_file_name, corpus_file_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D35ogrPXl_0m"
      },
      "source": [
        "### AIHUB 뉴스기사 기계독해 데이터"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=577)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "858DLKaUmEK2"
      },
      "outputs": [],
      "source": [
        "def json_file_name_list(path_list):\n",
        "    for i in path_list:\n",
        "        if 'rain' in i:\n",
        "            train_file_name = glob(i, recursive = True)\n",
        "        elif 'alid' in i:  \n",
        "            valid_file_name = glob(i, recursive = True)\n",
        "    return train_file_name, valid_file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QrXxvXW1mFeF"
      },
      "outputs": [],
      "source": [
        "path_list = ['/content/drive/MyDrive/AIHUB/AIHUB_뉴스 기사 기계독해 데이터/Training/'+ '/**/*.json', \n",
        "             '/content/drive/MyDrive/AIHUB/AIHUB_뉴스 기사 기계독해 데이터/Validation/'+ '/**/*.json']\n",
        "train_file_name, valid_file_name = json_file_name_list(path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "M4kCMiLNlWqN"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(file_name_list, corpus_file_name):\n",
        "\n",
        "  sentence_list = []\n",
        "\n",
        "  for i in range(len(file_name_list)):\n",
        "    with open(train_file_name[2], 'r', encoding='utf-8-sig', errors='ignore') as one_json_file:\n",
        "      one_json_sample = json.load(one_json_file, strict=False)\n",
        "      \n",
        "    for j in one_json_sample['data']:\n",
        "        for sentence in kss.split_sentences(j['paragraphs'][0]['context']):\n",
        "            if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                sentence_list.append(sentence)  \n",
        "\n",
        "  with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:       \n",
        "      fp.write(\"\\n\".join(sentence_list)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tOWznqqRmUuN"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_news_article_machine_reading_data.txt\"\n",
        "make_corpus_txt(train_file_name, corpus_file_name)\n",
        "make_corpus_txt(valid_file_name, corpus_file_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
