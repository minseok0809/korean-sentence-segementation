{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZxXpSOQQ2zr"
      },
      "source": [
        "## AI Hub Json Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVFMLU7QyrQ"
      },
      "source": [
        "### Development Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9OWQBifJhyy"
      },
      "outputs": [],
      "source": [
        "%pip install kss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXb6le51MmWM"
      },
      "outputs": [],
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x5YUuzoQQyFn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import kss\n",
        "import json\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import konlpy\n",
        "from konlpy.tag import Mecab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZKPC50uQ0cE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya2TvQE9RBjj"
      },
      "source": [
        "### AIHUB 대규모 구매도서 기반 한국어 말뭉치 데이터"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=653)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AMiX4kNCRB1i"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/drive/MyDrive/AIHUB/AIHUB_대규모 구매도서 기반 한국어 말뭉치 데이터/000_DATA.tsv'\n",
        "dataset = pd.read_csv(file_name, sep = '\\t', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iea04A3KRB4C"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(dataset, corpus_file_name):\n",
        "    sentence_list = []\n",
        "    for i in dataset['contents1']:\n",
        "        for sentence in kss.split_sentences(i):\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]', sentence[0])) == False:  \n",
        "                sentence_list.append(sentence)  \n",
        "\n",
        "    with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:      \n",
        "        fp.write(\"\\n\".join(sentence_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MBkDzMx2RB7i"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_korean_corpus_data_based_on_large_scale_purchase_books.txt\"\n",
        "make_corpus_txt(dataset, corpus_file_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AILDBxkQIp7c"
      },
      "source": [
        "### AIHUB 문서요약 텍스트"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=97)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I1aJvzjyPF2m"
      },
      "outputs": [],
      "source": [
        "def json_file_name_list(path_list):\n",
        "    for i in path_list:\n",
        "        if 'rain' in i:\n",
        "            train_file_name = glob(i, recursive = True)\n",
        "        elif 'alid' in i:  \n",
        "            valid_file_name = glob(i, recursive = True)\n",
        "    return train_file_name, valid_file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tSltwRg8aNtm"
      },
      "outputs": [],
      "source": [
        "path_list = ['/content/drive/MyDrive/AIHUB/AIHUB_문서요약 텍스트/Training/'+ '/**/*.json', \n",
        "             '/content/drive/MyDrive/AIHUB/AIHUB_문서요약 텍스트/Validatoin/'+ '/**/*.json']\n",
        "train_file_name, valid_file_name = json_file_name_list(path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JEzGiByoRJsE"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(file_name_list, corpus_file_name):\n",
        "\n",
        "  sentence_list = []\n",
        "  for i in range(len(file_name_list)):\n",
        "    with open(file_name_list[i], 'r', encoding='utf-8') as one_json_file:\n",
        "      one_json_sample = json.load(one_json_file)\n",
        "\n",
        "      for j in one_json_sample['documents']:\n",
        "        for k in j['text'][0]:\n",
        "          sentence = k['sentence']\n",
        "          if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[(]', sentence[0])) == False and \\\n",
        "          bool(re.match(r'[가-힣]+.', sentence[:2])) == False and \\\n",
        "          bool(re.match(r'[+[0-9]+]', sentence[:3])) == False and \"[다수의견]\" not in sentence:\n",
        "            sentence_list.append(sentence)  \n",
        "\n",
        "    with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:      \n",
        "        fp.write(\"\\n\".join(sentence_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "T1QpP1muacuo"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_document_summary_text.txt\"\n",
        "make_corpus_txt(train_file_name, corpus_file_name)\n",
        "make_corpus_txt(valid_file_name, corpus_file_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6vxetokib87T"
      },
      "source": [
        "### AIHUB 논문자료 요약"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Source](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWcxiIFZb1Kl"
      },
      "outputs": [],
      "source": [
        "def json_file_name_list(path_list):\n",
        "    for i in path_list:\n",
        "        if 'rain' in i:\n",
        "            train_file_name = glob(i, recursive = True)\n",
        "        elif 'alid' in i:  \n",
        "            valid_file_name = glob(i, recursive = True)\n",
        "    return train_file_name, valid_file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HdpQ5aGdcA-G"
      },
      "outputs": [],
      "source": [
        "path_list = ['/content/drive/MyDrive/AIHUB/AIHUB_논문자료 요약/Training/'+ '/**/*.json', \n",
        "             '/content/drive/MyDrive/AIHUB/AIHUB_논문자료 요약/Validatoin/'+ '/**/*.json']\n",
        "train_file_name, valid_file_name = json_file_name_list(path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1PTLFr6liirj"
      },
      "outputs": [],
      "source": [
        "def make_corpus_txt(file_name_list, corpus_file_name):\n",
        "\n",
        "  sentence_list = []\n",
        "\n",
        "  for i in range(len(file_name_list)):\n",
        "    with open(file_name_list[i], 'r', encoding='utf-8') as one_json_file:\n",
        "      one_json_sample = json.load(one_json_file)\n",
        "    \n",
        "    for j in one_json_sample['data']:\n",
        "      try:\n",
        "        summary_entire = j['summary_entire'][0]\n",
        "      except KeyError:\n",
        "        pass \n",
        "      finally: \n",
        "        summary_section = j['summary_section'][0]\n",
        "\n",
        "      if '논문/논문요약' in file_name_list[i]:\n",
        "        try:\n",
        "          for sentence in kss.split_sentences(summary_entire['orginal_text']):\n",
        "              if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                  sentence_list.append(sentence)  \n",
        "        except KeyError:\n",
        "          pass            \n",
        "\n",
        "      if 1 > 0:    \n",
        "        try:    \n",
        "          summary_entire = j['summary_entire'][0]\n",
        "          for sentence in kss.split_sentences(summary_entire['summary_text']):\n",
        "              if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                  sentence_list.append(sentence)  \n",
        "        except KeyError:\n",
        "          pass\n",
        "\n",
        "        finally:\n",
        "          for sentence in kss.split_sentences(summary_section['orginal_text']):\n",
        "              if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                  sentence_list.append(sentence)  \n",
        "          for sentence in kss.split_sentences(summary_section['summary_text']):\n",
        "              if bool(re.match(r'[.]|[,]|[◆]|[◇]|[△]|[▲]|[▽]|[▼]|[▷]|[▶]|[<]|[>]|[0-9]|[《]|[/]|[○]|[-]| ]', sentence[0])) == False:\n",
        "                  sentence_list.append(sentence) \n",
        "       \n",
        "  with open(os.path.join('/content/drive/MyDrive/AIHUB/AIHUB_corpus/', corpus_file_name), 'a', encoding=\"UTF-8\") as fp:       \n",
        "      fp.write(\"\\n\".join(sentence_list)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68LfWGvxcBCr"
      },
      "outputs": [],
      "source": [
        "corpus_file_name = \"AIHUB_summary_of_thesis_materials.txt\"\n",
        "make_corpus_txt(train_file_name, corpus_file_name)\n",
        "make_corpus_txt(valid_file_name, corpus_file_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
