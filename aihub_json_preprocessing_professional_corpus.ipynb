{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHub Json Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kss==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS Argument Error: Restart Jupyter Kernel Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-mecab-ko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 3.7.3 matches python-mecab-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import kss\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from mecab import MeCab\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AIHUB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "    \n",
    "    path_list = sorted(path_list, reverse=False)\n",
    "    path_list = sorted(path_list, key=len)\n",
    "    \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_source_file_list(l, n): \n",
    "    \n",
    "  for i in range(0, len(l), n): \n",
    "    yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_file_path_list(path_list):\n",
    "    \n",
    "    file_path  = [glob(i, recursive = True) for i in path_list][0]\n",
    "    file_path = sorted_list(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_json_file_path_list(path_list):\n",
    "\n",
    "  train_file_path, valid_file_path = [glob(i, recursive = True) if 'rain' in i\n",
    "                                      else glob(i, recursive = True)\n",
    "                                      for i in path_list]\n",
    "\n",
    "  train_file_path = sorted_list(train_file_path)\n",
    "  valid_file_path = sorted_list(valid_file_path)\n",
    "    \n",
    "  return train_file_path, valid_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_path_list(source_file_nested_list, folder_corpus_type_path):\n",
    "\n",
    "  text_file_path_list = [folder_corpus_type_path + str(i) + \".txt\"\n",
    "                              for i in range(len(source_file_nested_list))]\n",
    "    \n",
    "  return text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    train_file_path, valid_file_path = train_valid_json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(train_file_path) + len(valid_file_path)\n",
    "    print(\"The number of file:\", the_number_of_json_file)\n",
    "    \n",
    "    train_text_file_path_list = txt_file_path_list(train_file_path, txt_path_list[0])\n",
    "    valid_text_file_path_list = txt_file_path_list(valid_file_path, txt_path_list[1])\n",
    "\n",
    "    return train_file_path, valid_file_path, train_text_file_path_list, valid_text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_txt_file_path_list(json_path_list, txt_path_list):\n",
    "\n",
    "    file_path = json_file_path_list(json_path_list)\n",
    "    \n",
    "    the_number_of_json_file = len(file_path) \n",
    "    print(\"The number of file:\", the_number_of_json_file)\n",
    "    \n",
    "    text_file_path_list = txt_file_path_list(file_path, txt_path_list[0])\n",
    "\n",
    "    return file_path, text_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "    \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "\n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence)    \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "                preprocessing_sentence_list.append(sentence)\n",
    "                \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "   \n",
    "    source = re.sub(r'제\\d{1,}조의\\d{1,}\\([^)]+\\)', \"\", source)\n",
    "    source = re.sub(r'제\\d{1,}조\\([^)]+\\)', \"\",source)\n",
    "    sources = re.split(r\"[0-9]+[.]|①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩|⑪|⑫|⑬|⑭|⑮\", source)\n",
    "    # '제n조의n항(text)', '제n조(text)' 꼴 제거\n",
    "    # 숫자.(1. 2. ...), 원숫자(①, ②, ③, ...)를 기준으로 문장 분리\n",
    "    \n",
    "    for source in sources:\n",
    "    \n",
    "        for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                            num_workers=32):\n",
    "        # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "        # Formal articles (wiki, news, essays): recommend to False\n",
    "\n",
    "            if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮]\", sentence[0]) is not None and \\\n",
    "                bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "                len(sentence.split()) > 5:\n",
    "                # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, +, \n",
    "                # ①, ②, ③, ④, ⑤, ⑥, ⑦, ⑧, ⑨, ⑩, ⑪, ⑫, ⑬, ⑭, ⑮ 제외) 제외\n",
    "                # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "                # 다섯 어절 이하 문장 제외\n",
    "\n",
    "                sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "                # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "                sentence = sentence.strip()\n",
    "                # strip으로 앞뒤 공백 제거\n",
    "\n",
    "                total_length = len(sentence.replace(\" \" , \"\"))\n",
    "                hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "                hangeul_ratio = hangeul_length / total_length\n",
    "                if hangeul_ratio >= 0.5:\n",
    "                # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "                    preprocessing_sentence_list.append(sentence)\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIHUB 전문분야 말뭉치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=110)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Json Text to TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['AIHUB_전문분야 말뭉치/Training/'+ '/**/*.json', \n",
    "                  'AIHUB_전문분야 말뭉치/Validation/'+ '/**/*.json']\n",
    "txt_path_list = [\"exploration/professional_corpus_pro/AIHUB_professional_corpus_train_\", \n",
    "                 \"exploration/professional_corpus_pro/AIHUB_professional_corpus_valid_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of file: 39\n"
     ]
    }
   ],
   "source": [
    "train_file_list, valid_file_list, train_text_file_path_list, valid_text_file_path_list = \\\n",
    "    make_train_valid_json_txt_file_path_list(json_path_list, txt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_index_df = pd.DataFrame(train_file_list, columns=['source_file_name'])\n",
    "source_file_index_df.to_excel(\"source_file_index/professional_corpus_source_train_file_index.xlsx\", index=False)\n",
    "\n",
    "source_file_index_df = pd.DataFrame(valid_file_list, columns=['source_file_name'])\n",
    "source_file_index_df.to_excel(\"source_file_index/professional_corpus_source_valid_file_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_source_list(source_file, json_sample):\n",
    "    \n",
    "    if '논문_' in source_file:\n",
    "        source_list = list(pd.DataFrame(json_sample['data'])['text'])\n",
    "        \n",
    "    elif '특허_z' in source_file:\n",
    "        source_df = pd.DataFrame(json_sample['data'])\n",
    "        source_list = list(source_df[source_df['attr'] == 'abs']['text'])\n",
    "\n",
    "    elif '특허_0' in source_file or '특허_1' in source_file:\n",
    "        source_df = pd.DataFrame(json_sample['data'])\n",
    "        source_dict = dict(source_df[source_df['attr'] == '초록']['sentence'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='text').values[0])\n",
    "\n",
    "    elif '법령' in source_file or '판례' in source_file:\n",
    "        source_df = pd.DataFrame(json_sample['data'])\n",
    "        source_dict = dict(source_df['sentence'].explode())\n",
    "        source_json = pd.json_normalize(source_dict)\n",
    "        source_list = list(source_json.filter(regex='text').values[0])\n",
    "\n",
    "    return source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_txt_file_with_batch_list(source_file_list, batch_size):\n",
    "    \n",
    "    source_file_by_batch_df = pd.DataFrame({'File':[0], 'Length of Source List':[0],\n",
    "                                            'The Number of txt File':[0], \n",
    "                                            'Description':[0]})\n",
    "                                            \n",
    "    the_number_of_total_txt_file = 0\n",
    "    the_number_of_txt_file_list = []\n",
    "    \n",
    "    for i in range(len(source_file_list)):    \n",
    "        \n",
    "        source_file = source_file_list[i]        \n",
    "\n",
    "        with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "            one_json_sample = json.load(one_json_file) \n",
    "\n",
    "        try:\n",
    "            source_list = make_source_list(source_file, one_json_sample)\n",
    "            \n",
    "            the_number_of_txt_file = ((len(source_list) // batch_size) + 1) \n",
    "\n",
    "            if len(source_list) >= batch_size:\n",
    "                source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                                len(source_list), the_number_of_txt_file, \"\"]\n",
    "                the_number_of_txt_file_list.append(the_number_of_txt_file)\n",
    "                the_number_of_total_txt_file  += the_number_of_txt_file\n",
    "\n",
    "            elif len(source_list) < batch_size:\n",
    "                source_file_by_batch_df.loc[i] = [source_file,\n",
    "                                                len(source_list), the_number_of_txt_file,\n",
    "                                                \"not subject of batch. small source list.\"]\n",
    "                the_number_of_txt_file_list.append(1)\n",
    "                the_number_of_total_txt_file  += 1\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"The number of txt file:\", the_number_of_total_txt_file)\n",
    "    \n",
    "    if 'rain' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/professional_corpus_train.xlsx\", index=False)\n",
    "    elif 'alid' in source_file:\n",
    "        source_file_by_batch_df.to_excel(\"source_file_by_batch/professional_corpus_valid.xlsx\", index=False)\n",
    "    else:\n",
    "         source_file_by_batch_df.to_excel(\"source_file_by_batch/professional_corpus.xlsx\", index=False)\n",
    "    \n",
    "    return the_number_of_total_txt_file, the_number_of_txt_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsontext_to_txt_file_with_batch_list(source_file_list,\n",
    "                                    text_file_path_list,\n",
    "                                    batch_size, the_number_of_txt_file_list):\n",
    "\n",
    "  progress_length = sum(the_number_of_txt_file_list)\n",
    "  print(\"[Size]\")\n",
    "  print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "  print(\"\\n[Order]\")\n",
    "  pbar = tqdm(range(progress_length))\n",
    "  num = 0\n",
    "  \n",
    "  for i in range(len(source_file_list)):\n",
    "\n",
    "    source_file = source_file_list[i]\n",
    "\n",
    "    with open(source_file, 'r', encoding='utf-8') as one_json_file:\n",
    "      one_json_sample = json.load(one_json_file)\n",
    "\n",
    "    source_list = make_source_list(source_file, one_json_sample)\n",
    "    \n",
    "    n = batch_size\n",
    "    source_batch_list = list(divide_source_file_list(source_list, n))\n",
    "      \n",
    "    for source_list in source_batch_list:\n",
    "        num += 1\n",
    "        print(str(num), end=\" \")  \n",
    "      \n",
    "        if '법령' in source_file or '판례' in source_file:\n",
    "            with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \"_legal_text.txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "                fp.write(\"\\n\".join(source_list))   \n",
    "            num += 1  \n",
    "            pbar.n += 1\n",
    "            pbar.refresh()\n",
    "            time.sleep(0.01)  \n",
    "\n",
    "        else:\n",
    "            with open(os.path.join('AIHUB_corpus/' + text_file_path_list[i][:-4] + \"_\" + str(num) + \".txt\"), \"a\", encoding='utf-8') as fp:        \n",
    "                fp.write(\"\\n\".join(source_list))   \n",
    "            num += 1  \n",
    "            pbar.n += 1\n",
    "            pbar.refresh()\n",
    "            time.sleep(0.01)  \n",
    "  pbar.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1000\n",
      "The number of txt file: 809\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_train_txt_file, the_number_of_train_txt_file_list = count_number_of_txt_file_with_batch_list(train_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>File</th>\n",
       "      <th>Length of Source List</th>\n",
       "      <th>The Number of txt File</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_01.j...</td>\n",
       "      <td>5104</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_02.j...</td>\n",
       "      <td>5524</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_03.j...</td>\n",
       "      <td>4860</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_04.j...</td>\n",
       "      <td>5497</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_05.j...</td>\n",
       "      <td>5153</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_06.j...</td>\n",
       "      <td>4829</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_07.j...</td>\n",
       "      <td>4562</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_08.j...</td>\n",
       "      <td>5193</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_09.j...</td>\n",
       "      <td>5580</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_10.j...</td>\n",
       "      <td>5033</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_11.j...</td>\n",
       "      <td>5048</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_12.j...</td>\n",
       "      <td>4865</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_13.j...</td>\n",
       "      <td>4745</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_14.j...</td>\n",
       "      <td>5011</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_15.j...</td>\n",
       "      <td>4087</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_16.j...</td>\n",
       "      <td>4617</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\논문_1_0....</td>\n",
       "      <td>50000</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\논문_1_1....</td>\n",
       "      <td>50354</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_1....</td>\n",
       "      <td>11009</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_2....</td>\n",
       "      <td>9680</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_3....</td>\n",
       "      <td>8874</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_4....</td>\n",
       "      <td>2353</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>72857</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>71311</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>70948</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>60475</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>60389</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...</td>\n",
       "      <td>76293</td>\n",
       "      <td>77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_10...</td>\n",
       "      <td>31960</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_10...</td>\n",
       "      <td>26802</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_13...</td>\n",
       "      <td>26267</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_21...</td>\n",
       "      <td>25153</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_30...</td>\n",
       "      <td>28534</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_40...</td>\n",
       "      <td>27751</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               File  \\\n",
       "0            0  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_01.j...   \n",
       "1            1  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_02.j...   \n",
       "2            2  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_03.j...   \n",
       "3            3  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_04.j...   \n",
       "4            4  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_05.j...   \n",
       "5            5  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_06.j...   \n",
       "6            6  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_07.j...   \n",
       "7            7  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_08.j...   \n",
       "8            8  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_09.j...   \n",
       "9            9  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_10.j...   \n",
       "10          10  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_11.j...   \n",
       "11          11  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_12.j...   \n",
       "12          12  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_13.j...   \n",
       "13          13  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_14.j...   \n",
       "14          14  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_15.j...   \n",
       "15          15  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_16.j...   \n",
       "16          16  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\논문_1_0....   \n",
       "17          17  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\논문_1_1....   \n",
       "18          18  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_1....   \n",
       "19          19  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_2....   \n",
       "20          20  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_3....   \n",
       "21          21  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\특허_z_4....   \n",
       "22          22  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "23          23  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "24          24  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "25          25  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "26          26  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "27          27  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\판례_case...   \n",
       "28          28  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_10...   \n",
       "29          29  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_10...   \n",
       "30          30  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_13...   \n",
       "31          31  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_21...   \n",
       "32          32  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_30...   \n",
       "33          33  AIHUB_전문분야 말뭉치/Training\\[원천]전문분야_train\\법령_l_40...   \n",
       "\n",
       "    Length of Source List  The Number of txt File  Description  \n",
       "0                    5104                       6          NaN  \n",
       "1                    5524                       6          NaN  \n",
       "2                    4860                       5          NaN  \n",
       "3                    5497                       6          NaN  \n",
       "4                    5153                       6          NaN  \n",
       "5                    4829                       5          NaN  \n",
       "6                    4562                       5          NaN  \n",
       "7                    5193                       6          NaN  \n",
       "8                    5580                       6          NaN  \n",
       "9                    5033                       6          NaN  \n",
       "10                   5048                       6          NaN  \n",
       "11                   4865                       5          NaN  \n",
       "12                   4745                       5          NaN  \n",
       "13                   5011                       6          NaN  \n",
       "14                   4087                       5          NaN  \n",
       "15                   4617                       5          NaN  \n",
       "16                  50000                      51          NaN  \n",
       "17                  50354                      51          NaN  \n",
       "18                  11009                      12          NaN  \n",
       "19                   9680                      10          NaN  \n",
       "20                   8874                       9          NaN  \n",
       "21                   2353                       3          NaN  \n",
       "22                  72857                      73          NaN  \n",
       "23                  71311                      72          NaN  \n",
       "24                  70948                      71          NaN  \n",
       "25                  60475                      61          NaN  \n",
       "26                  60389                      61          NaN  \n",
       "27                  76293                      77          NaN  \n",
       "28                  31960                      32          NaN  \n",
       "29                  26802                      27          NaN  \n",
       "30                  26267                      27          NaN  \n",
       "31                  25153                      26          NaN  \n",
       "32                  28534                      29          NaN  \n",
       "33                  27751                      28          NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_by_batch_train_df = pd.read_excel('source_file_by_batch/professional_corpus_train.xlsx', engine='openpyxl')  \n",
    "source_file_by_batch_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(train_file_list, train_text_file_path_list,\n",
    "                batch_size, the_number_of_train_txt_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1000\n",
      "The number of txt file: 113\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "the_number_of_valid_txt_file, the_number_of_valid_txt_file_list = count_number_of_txt_file_with_batch_list(valid_file_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>File</th>\n",
       "      <th>Length of Source List</th>\n",
       "      <th>The Number of txt File</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\특허_19...</td>\n",
       "      <td>5131</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\논문_1_...</td>\n",
       "      <td>12544</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\판례_ca...</td>\n",
       "      <td>69613</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\법령_l_...</td>\n",
       "      <td>23592</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               File  \\\n",
       "0           0  AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\특허_19...   \n",
       "1           2  AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\논문_1_...   \n",
       "2           3  AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\판례_ca...   \n",
       "3           4  AIHUB_전문분야 말뭉치/Validation\\[원천]전문분야_valid\\법령_l_...   \n",
       "\n",
       "   Length of Source List  The Number of txt File  Description  \n",
       "0                   5131                       6          NaN  \n",
       "1                  12544                      13          NaN  \n",
       "2                  69613                      70          NaN  \n",
       "3                  23592                      24          NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_by_batch_valid_df = pd.read_excel('source_file_by_batch/professional_corpus_valid.xlsx', engine='openpyxl')  \n",
    "source_file_by_batch_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_txt_file_with_batch_list(valid_file_list, valid_text_file_path_list,\n",
    "                batch_size, the_number_of_valid_txt_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def professional_corpus_post_txt_file_path_list(corpus_path_list):\n",
    "   \n",
    "  post_corpus_path_list = [corpus_file.replace(\"professional_corpus_pro\", \"professional_corpus_post\")\n",
    "                      for corpus_file in corpus_path_list]\n",
    "\n",
    "  return post_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pro_post_txt_file_path_list(pro_corpus_path):\n",
    "    \n",
    "    pro_total_corpus_path_list = glob(pro_corpus_path)\n",
    "    pro_total_corpus_path_list = sorted_list(pro_total_corpus_path_list)\n",
    "    post_total_corpus_path_list = professional_corpus_post_txt_file_path_list(pro_total_corpus_path_list)\n",
    "\n",
    "    return pro_total_corpus_path_list, post_total_corpus_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_corpus_path = \"AIHUB_corpus/exploration/professional_corpus_pro/AIHUB_professional_corpus_\" + \"*.txt\"\n",
    "pro_total_corpus_path_list, post_total_corpus_path_list = make_pro_post_txt_file_path_list(pro_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pro_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 발명에 따른 표시 장치는 기판, 기판 위에 형성되어 있는 제1 보조 전극, 제1 보조 전극과 위에 형성되어 있고, 제1 입력 단자, 제1 출력 단자 및 제1 제어 단자를 가지는 제1 박막 트랜지스터, 제1 박막 트랜지스터와 연결되어 있는 화소 전극을 포함하고, 제1 보조 전극은 제1 박막 트랜지스터와 중첩하고, 제1 입력 단자와 전기적으로 연결되어 있다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines() \n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:\n",
    "           line_list.append(line)\n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 발명에 따른 표시 장치는 기판, 기판 위에 형성되어 있는 제1 보조 전극, 제1 보조 전극과 위에 형성되어 있고, 제1 입력 단자, 제1 출력 단자 및 제1 제어 단자를 가지는 제1 박막 트랜지스터, 제1 박막 트랜지스터와 연결되어 있는 화소 전극을 포함하고, 제1 보조 전극은 제1 박막 트랜지스터와 중첩하고, 제1 입력 단자와 전기적으로 연결되어 있다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_list = []\n",
    "line_num = 0\n",
    "with open(pro_total_corpus_path_list[0], 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line_num += 1\n",
    "        if line_num <= 1:  \n",
    "            sentences = formal_preprocessing_text(line)\n",
    "            for sentence in sentences:\n",
    "                line_list.append(sentence) \n",
    "            \n",
    "for line in line_list:\n",
    "    print(line, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 19:56:45,726\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus = 4)\n",
    "\n",
    "@ray.remote\n",
    "def formal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    \n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "        \n",
    "    for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "    # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "    # Formal articles (wiki, news, essays): recommend to False\n",
    "    \n",
    "\n",
    "        if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎]\", sentence[0]) is not None and \\\n",
    "            bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "            len(sentence.split()) > 5:\n",
    "            # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, + 제외\n",
    "            # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "            # 다섯 어절 이하 문장 제외\n",
    "\n",
    "\n",
    "            if ']' in sentence and '[' not in sentence:\n",
    "                sentence  = re.sub(r\".*?]\", \"\", sentence)    \n",
    "            # 중괄호 앞에 있는 '성명/직함]' 형태 제거\n",
    "\n",
    "\n",
    "            sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "            # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            # strip으로 앞뒤 공백 제거\n",
    "\n",
    "            total_length = len(sentence.replace(\" \" , \"\"))\n",
    "            hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "            hangeul_ratio = hangeul_length / total_length\n",
    "            if hangeul_ratio >= 0.5:\n",
    "            # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "\n",
    "                for sentence2 in kss.split_sentences(sentence, use_heuristic=False,\n",
    "                                        num_workers=32):\n",
    "                    for sentence3 in kss.split_sentences(sentence2, use_heuristic=False,\n",
    "                                                         num_workers=32):\n",
    "                        preprocessing_sentence_list.append(sentence3)\n",
    "\n",
    "            # 마지막에 KSS(Korean Sentence Segmentation)로 문장 분리 2번 실행\n",
    "\n",
    "  \n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def legal_preprocessing_text(source):\n",
    "    preprocessing_sentence_list = []\n",
    "    source = source.strip()\n",
    "    # strip으로 앞뒤 공백 제거\n",
    "\n",
    "    source = re.sub(r\"\\[.*?\\]|\\{.*?\\}\", \"\", source)\n",
    "    # 기타 괄호 제거할 시 괄호 내부에 모든 텍스트 제거\n",
    "\n",
    "    try:\n",
    "        bracket_form = re.compile('\\(([^)]+)')\n",
    "        text_in_small_bracket = bracket_form.findall(source)\n",
    "    \n",
    "    \n",
    "        if type(text_in_small_bracket) == str:\n",
    "\n",
    "            text = text_in_small_bracket\n",
    "\n",
    "            text_size = len(text)\n",
    "            last_index = source.find(text) + len(text)\n",
    "            if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "            if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                small_bracket = \"(\" + text + \")\"\n",
    "                source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "        elif type(text_in_small_bracket) == list:\n",
    "\n",
    "            for text in text_in_small_bracket:\n",
    "\n",
    "                text_size = len(text)\n",
    "                last_index = source.find(text) + len(text)\n",
    "                if len(source) >= last_index+1 and source[last_index-text_size-1] == '(' and source[last_index+1] == '.':\n",
    "                    source = source.replace(source[last_index-text_size-1 : last_index+1] + \".\", \".\")\n",
    "\n",
    "                if len(text.split()) > 5 and bool(re.match(r'[.]|[!]|[?]', text[-1])) == True:\n",
    "                    small_bracket = \"(\" + text + \")\"\n",
    "                    source = source.replace(small_bracket, \" \" + text + \" \")    \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # 마침표(.) 앞에 소괄호')'가 있을시 소괄호 제거와 함께 소괄호 내부 텍스트 제거\n",
    "        # 소괄호 내부 텍스트가 5어절 이상이고 끝이 온점(.). 느낌표(!). 물음표(?)일 떼 소괄호 제거\n",
    "        \n",
    "    \n",
    "    if bool(re.match(r'[가나다라마바사아자차카타파하]+[.]', source[:2])) == True:\n",
    "        source = source.replace(source[:2], \"\")\n",
    "        \n",
    "    source = re.sub(r' [가나다라마바사아자차카타파하]+[.]', \"\", source)\n",
    "    # '가.', '나.', ... 형태의 문자열 제거 \n",
    "   \n",
    "    source = re.sub(r'제\\d{1,}조의\\d{1,}\\([^)]+\\)', \"\", source)\n",
    "    source = re.sub(r'제\\d{1,}조\\([^)]+\\)', \"\",source)\n",
    "    sources = re.split(r\"[0-9]+[.]|①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩|⑪|⑫|⑬|⑭|⑮\", source)\n",
    "    # '제n조의n항(text)', '제n조(text)' 꼴 제거\n",
    "    # 숫자.(1. 2. ...), 원숫자(①, ②, ③, ...)를 기준으로 문장 분리\n",
    "    \n",
    "    for source in sources:\n",
    "    \n",
    "        for sentence in kss.split_sentences(source, use_heuristic=False,\n",
    "                                            num_workers=32):\n",
    "        # KSS(Korean Sentence Segmentation)로 문장 분리 \n",
    "        # Formal articles (wiki, news, essays): recommend to False\n",
    "\n",
    "            if re.search(\"^[A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮]\", sentence[0]) is not None and \\\n",
    "                bool(re.match(r'[.]|[!]|[?]', sentence[-1])) == True and \\\n",
    "                len(sentence.split()) > 5:\n",
    "                # 문장의 시작이 특수문자인 문장(영어 대소문자, 한글, 한자, 숫자, -, +, \n",
    "                # ①, ②, ③, ④, ⑤, ⑥, ⑦, ⑧, ⑨, ⑩, ⑪, ⑫, ⑬, ⑭, ⑮ 제외) 제외\n",
    "                # 문장의 끝이 온점(.). 느낌표(!). 물음표(?)가 아닌 문장 제외\n",
    "                # 다섯 어절 이하 문장 제외\n",
    "\n",
    "                sentence = re.sub(r\"[^A-Za-z0-9ㄱ-ㅎ가-힣一-鿕㐀-䶵豈-龎()+-.,]\", \" \", sentence)\n",
    "                # 특수문자 제거(영어 대소문자, 한글, 한자, 숫자, -, +, 소괄호, 마침표, 쉼표, 제외)\n",
    "\n",
    "                sentence = sentence.strip()\n",
    "                # strip으로 앞뒤 공백 제거\n",
    "\n",
    "                total_length = len(sentence.replace(\" \" , \"\"))\n",
    "                hangeul_length = len(re.sub(r\"[^ㄱ-ㅣ가-힣\\s]\", \"\", sentence.replace(\" \" , \"\")))\n",
    "                hangeul_ratio = hangeul_length / total_length\n",
    "                if hangeul_ratio >= 0.5:\n",
    "                # 한글이 아닌 문자열이 50% 이상이 넘은 문장 제외\n",
    "                    preprocessing_sentence_list.append(sentence)\n",
    "\n",
    "    return preprocessing_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "\n",
    "    progress_length = len(pro_total_corpus_path_list)   \n",
    "    print(\"[Size]\")\n",
    "    print(\"The number of preprocessing corpus: \" + str(progress_length))\n",
    "    print(\"\\n[Order]\")\n",
    "    pbar = tqdm(range(progress_length))\n",
    "    num = 0\n",
    "    process_num = 10 \n",
    "    for pro, post in zip(pro_total_corpus_path_list, post_total_corpus_path_list):\n",
    "        \n",
    "        sentence_list = []\n",
    "        \n",
    "        with open(pro, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines() \n",
    "            nested_lines_num = len(lines) // process_num\n",
    "            for i in range(nested_lines_num - 1):\n",
    "                start_line = process_num * i\n",
    "                end_line = process_num * (i+1)\n",
    "                \n",
    "                if 'legal_text' in pro:\n",
    "                    futures = [legal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [legal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "                        \n",
    "                else:\n",
    "                    futures = [formal_preprocessing_text.remote(lines[start_line:end_line][j]) for j in range(process_num)]\n",
    "                    results = ray.get(futures)\n",
    "                    if i == nested_lines_num - 2:\n",
    "                        futures = [formal_preprocessing_text.remote(lines[end_line:][j]) for j in range(len(lines) - end_line)]\n",
    "                        results = ray.get(futures)\n",
    "                    \n",
    "                sentences = list(chain.from_iterable(results))\n",
    "                sentence_list.append(sentences)\n",
    "        \n",
    "        sentence_list = list(chain.from_iterable(sentence_list))\n",
    "\n",
    "        with open(post, 'a', encoding='utf-8') as fp:\n",
    "            fp.write(\"\\n\".join(sentence_list))\n",
    "        pbar.n += 1\n",
    "        pbar.refresh()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    pbar.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_txt(pro_total_corpus_path_list, post_total_corpus_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path,\n",
    "                                      deduplicate_corpus_path):\n",
    "    \n",
    "    corpus_list = glob(preprocessing_corpus_path)\n",
    "    corpus_list = sorted_list(corpus_list)\n",
    "    \n",
    "    with open(merge_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for corpus in corpus_list:\n",
    "            with open(corpus, encoding='utf-8') as text:\n",
    "                for line in text:\n",
    "                    f.write(line)\n",
    "                    \n",
    "    with open(deduplicate_corpus_path, 'w', encoding='utf-8') as f1:\n",
    "        with open(merge_corpus_path, encoding='utf-8') as f2:\n",
    "            lines = f2.read().splitlines()\n",
    "            single_sentence_dict = dict.fromkeys(lines)\n",
    "            single_sentence_list = list(single_sentence_dict)\n",
    "            f1.write(\"\\n\".join(single_sentence_list))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_corpus_path = \"AIHUB_corpus/exploration/professional_corpus_source_post/AIHUB_professional_corpus_source_\" +\"*.txt\"\n",
    "merge_corpus_path = 'AIHUB_corpus/duplicate/AIHUB_professional_corpus_source.txt'\n",
    "deduplicate_corpus_path = 'AIHUB_corpus/AIHUB_professional_corpus_source.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_deduplicate_corpus_txt(preprocessing_corpus_path, merge_corpus_path, deduplicate_corpus_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpus_06",
   "language": "python",
   "name": "corpus_06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
